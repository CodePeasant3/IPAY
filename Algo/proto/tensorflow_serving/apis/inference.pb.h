// Generated by the protocol buffer compiler.  DO NOT EDIT!
// NO CHECKED-IN PROTOBUF GENCODE
// source: tensorflow_serving/apis/inference.proto
// Protobuf C++ Version: 5.29.0

#ifndef tensorflow_5fserving_2fapis_2finference_2eproto_2epb_2eh
#define tensorflow_5fserving_2fapis_2finference_2eproto_2epb_2eh

#include <limits>
#include <string>
#include <type_traits>
#include <utility>

#include "google/protobuf/runtime_version.h"
#if PROTOBUF_VERSION != 5029000
#error "Protobuf C++ gencode is built with an incompatible version of"
#error "Protobuf C++ headers/runtime. See"
#error "https://protobuf.dev/support/cross-version-runtime-guarantee/#cpp"
#endif
#include "google/protobuf/io/coded_stream.h"
#include "google/protobuf/arena.h"
#include "google/protobuf/arenastring.h"
#include "google/protobuf/generated_message_tctable_decl.h"
#include "google/protobuf/generated_message_util.h"
#include "google/protobuf/metadata_lite.h"
#include "google/protobuf/generated_message_reflection.h"
#include "google/protobuf/message.h"
#include "google/protobuf/message_lite.h"
#include "google/protobuf/repeated_field.h"  // IWYU pragma: export
#include "google/protobuf/extension_set.h"  // IWYU pragma: export
#include "google/protobuf/unknown_field_set.h"
#include "tensorflow_serving/apis/classification.pb.h"
#include "tensorflow_serving/apis/input.pb.h"
#include "tensorflow_serving/apis/model.pb.h"
#include "tensorflow_serving/apis/regression.pb.h"
// @@protoc_insertion_point(includes)

// Must be included last.
#include "google/protobuf/port_def.inc"

#define PROTOBUF_INTERNAL_EXPORT_tensorflow_5fserving_2fapis_2finference_2eproto

namespace google {
namespace protobuf {
namespace internal {
template <typename T>
::absl::string_view GetAnyMessageName();
}  // namespace internal
}  // namespace protobuf
}  // namespace google

// Internal implementation detail -- do not use these members.
struct TableStruct_tensorflow_5fserving_2fapis_2finference_2eproto {
  static const ::uint32_t offsets[];
};
extern const ::google::protobuf::internal::DescriptorTable
    descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto;
namespace tensorflow {
namespace serving {
class InferenceResult;
struct InferenceResultDefaultTypeInternal;
extern InferenceResultDefaultTypeInternal _InferenceResult_default_instance_;
class InferenceTask;
struct InferenceTaskDefaultTypeInternal;
extern InferenceTaskDefaultTypeInternal _InferenceTask_default_instance_;
class MultiInferenceRequest;
struct MultiInferenceRequestDefaultTypeInternal;
extern MultiInferenceRequestDefaultTypeInternal _MultiInferenceRequest_default_instance_;
class MultiInferenceResponse;
struct MultiInferenceResponseDefaultTypeInternal;
extern MultiInferenceResponseDefaultTypeInternal _MultiInferenceResponse_default_instance_;
}  // namespace serving
}  // namespace tensorflow
namespace google {
namespace protobuf {
}  // namespace protobuf
}  // namespace google

namespace tensorflow {
namespace serving {

// ===================================================================


// -------------------------------------------------------------------

class InferenceTask final
    : public ::google::protobuf::Message
/* @@protoc_insertion_point(class_definition:tensorflow.serving.InferenceTask) */ {
 public:
  inline InferenceTask() : InferenceTask(nullptr) {}
  ~InferenceTask() PROTOBUF_FINAL;

#if defined(PROTOBUF_CUSTOM_VTABLE)
  void operator delete(InferenceTask* msg, std::destroying_delete_t) {
    SharedDtor(*msg);
    ::google::protobuf::internal::SizedDelete(msg, sizeof(InferenceTask));
  }
#endif

  template <typename = void>
  explicit PROTOBUF_CONSTEXPR InferenceTask(
      ::google::protobuf::internal::ConstantInitialized);

  inline InferenceTask(const InferenceTask& from) : InferenceTask(nullptr, from) {}
  inline InferenceTask(InferenceTask&& from) noexcept
      : InferenceTask(nullptr, std::move(from)) {}
  inline InferenceTask& operator=(const InferenceTask& from) {
    CopyFrom(from);
    return *this;
  }
  inline InferenceTask& operator=(InferenceTask&& from) noexcept {
    if (this == &from) return *this;
    if (::google::protobuf::internal::CanMoveWithInternalSwap(GetArena(), from.GetArena())) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const
      ABSL_ATTRIBUTE_LIFETIME_BOUND {
    return _internal_metadata_.unknown_fields<::google::protobuf::UnknownFieldSet>(::google::protobuf::UnknownFieldSet::default_instance);
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields()
      ABSL_ATTRIBUTE_LIFETIME_BOUND {
    return _internal_metadata_.mutable_unknown_fields<::google::protobuf::UnknownFieldSet>();
  }

  static const ::google::protobuf::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::google::protobuf::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::google::protobuf::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const InferenceTask& default_instance() {
    return *internal_default_instance();
  }
  static inline const InferenceTask* internal_default_instance() {
    return reinterpret_cast<const InferenceTask*>(
        &_InferenceTask_default_instance_);
  }
  static constexpr int kIndexInFileMessages = 0;
  friend void swap(InferenceTask& a, InferenceTask& b) { a.Swap(&b); }
  inline void Swap(InferenceTask* other) {
    if (other == this) return;
    if (::google::protobuf::internal::CanUseInternalSwap(GetArena(), other->GetArena())) {
      InternalSwap(other);
    } else {
      ::google::protobuf::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(InferenceTask* other) {
    if (other == this) return;
    ABSL_DCHECK(GetArena() == other->GetArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  InferenceTask* New(::google::protobuf::Arena* arena = nullptr) const {
    return ::google::protobuf::Message::DefaultConstruct<InferenceTask>(arena);
  }
  using ::google::protobuf::Message::CopyFrom;
  void CopyFrom(const InferenceTask& from);
  using ::google::protobuf::Message::MergeFrom;
  void MergeFrom(const InferenceTask& from) { InferenceTask::MergeImpl(*this, from); }

  private:
  static void MergeImpl(
      ::google::protobuf::MessageLite& to_msg,
      const ::google::protobuf::MessageLite& from_msg);

  public:
  bool IsInitialized() const {
    return true;
  }
  ABSL_ATTRIBUTE_REINITIALIZES void Clear() PROTOBUF_FINAL;
  #if defined(PROTOBUF_CUSTOM_VTABLE)
  private:
  static ::size_t ByteSizeLong(const ::google::protobuf::MessageLite& msg);
  static ::uint8_t* _InternalSerialize(
      const MessageLite& msg, ::uint8_t* target,
      ::google::protobuf::io::EpsCopyOutputStream* stream);

  public:
  ::size_t ByteSizeLong() const { return ByteSizeLong(*this); }
  ::uint8_t* _InternalSerialize(
      ::uint8_t* target,
      ::google::protobuf::io::EpsCopyOutputStream* stream) const {
    return _InternalSerialize(*this, target, stream);
  }
  #else   // PROTOBUF_CUSTOM_VTABLE
  ::size_t ByteSizeLong() const final;
  ::uint8_t* _InternalSerialize(
      ::uint8_t* target,
      ::google::protobuf::io::EpsCopyOutputStream* stream) const final;
  #endif  // PROTOBUF_CUSTOM_VTABLE
  int GetCachedSize() const { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::google::protobuf::Arena* arena);
  static void SharedDtor(MessageLite& self);
  void InternalSwap(InferenceTask* other);
 private:
  template <typename T>
  friend ::absl::string_view(
      ::google::protobuf::internal::GetAnyMessageName)();
  static ::absl::string_view FullMessageName() { return "tensorflow.serving.InferenceTask"; }

 protected:
  explicit InferenceTask(::google::protobuf::Arena* arena);
  InferenceTask(::google::protobuf::Arena* arena, const InferenceTask& from);
  InferenceTask(::google::protobuf::Arena* arena, InferenceTask&& from) noexcept
      : InferenceTask(arena) {
    *this = ::std::move(from);
  }
  const ::google::protobuf::internal::ClassData* GetClassData() const PROTOBUF_FINAL;
  static void* PlacementNew_(const void*, void* mem,
                             ::google::protobuf::Arena* arena);
  static constexpr auto InternalNewImpl_();
  static const ::google::protobuf::internal::ClassDataFull _class_data_;

 public:
  ::google::protobuf::Metadata GetMetadata() const;
  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------
  enum : int {
    kMethodNameFieldNumber = 2,
    kModelSpecFieldNumber = 1,
  };
  // string method_name = 2;
  void clear_method_name() ;
  const std::string& method_name() const;
  template <typename Arg_ = const std::string&, typename... Args_>
  void set_method_name(Arg_&& arg, Args_... args);
  std::string* mutable_method_name();
  PROTOBUF_NODISCARD std::string* release_method_name();
  void set_allocated_method_name(std::string* value);

  private:
  const std::string& _internal_method_name() const;
  inline PROTOBUF_ALWAYS_INLINE void _internal_set_method_name(
      const std::string& value);
  std::string* _internal_mutable_method_name();

  public:
  // .tensorflow.serving.ModelSpec model_spec = 1;
  bool has_model_spec() const;
  void clear_model_spec() ;
  const ::tensorflow::serving::ModelSpec& model_spec() const;
  PROTOBUF_NODISCARD ::tensorflow::serving::ModelSpec* release_model_spec();
  ::tensorflow::serving::ModelSpec* mutable_model_spec();
  void set_allocated_model_spec(::tensorflow::serving::ModelSpec* value);
  void unsafe_arena_set_allocated_model_spec(::tensorflow::serving::ModelSpec* value);
  ::tensorflow::serving::ModelSpec* unsafe_arena_release_model_spec();

  private:
  const ::tensorflow::serving::ModelSpec& _internal_model_spec() const;
  ::tensorflow::serving::ModelSpec* _internal_mutable_model_spec();

  public:
  // @@protoc_insertion_point(class_scope:tensorflow.serving.InferenceTask)
 private:
  class _Internal;
  friend class ::google::protobuf::internal::TcParser;
  static const ::google::protobuf::internal::TcParseTable<
      1, 2, 1,
      52, 2>
      _table_;

  friend class ::google::protobuf::MessageLite;
  friend class ::google::protobuf::Arena;
  template <typename T>
  friend class ::google::protobuf::Arena::InternalHelper;
  using InternalArenaConstructable_ = void;
  using DestructorSkippable_ = void;
  struct Impl_ {
    inline explicit constexpr Impl_(
        ::google::protobuf::internal::ConstantInitialized) noexcept;
    inline explicit Impl_(::google::protobuf::internal::InternalVisibility visibility,
                          ::google::protobuf::Arena* arena);
    inline explicit Impl_(::google::protobuf::internal::InternalVisibility visibility,
                          ::google::protobuf::Arena* arena, const Impl_& from,
                          const InferenceTask& from_msg);
    ::google::protobuf::internal::HasBits<1> _has_bits_;
    ::google::protobuf::internal::CachedSize _cached_size_;
    ::google::protobuf::internal::ArenaStringPtr method_name_;
    ::tensorflow::serving::ModelSpec* model_spec_;
    PROTOBUF_TSAN_DECLARE_MEMBER
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_tensorflow_5fserving_2fapis_2finference_2eproto;
};
// -------------------------------------------------------------------

class InferenceResult final
    : public ::google::protobuf::Message
/* @@protoc_insertion_point(class_definition:tensorflow.serving.InferenceResult) */ {
 public:
  inline InferenceResult() : InferenceResult(nullptr) {}
  ~InferenceResult() PROTOBUF_FINAL;

#if defined(PROTOBUF_CUSTOM_VTABLE)
  void operator delete(InferenceResult* msg, std::destroying_delete_t) {
    SharedDtor(*msg);
    ::google::protobuf::internal::SizedDelete(msg, sizeof(InferenceResult));
  }
#endif

  template <typename = void>
  explicit PROTOBUF_CONSTEXPR InferenceResult(
      ::google::protobuf::internal::ConstantInitialized);

  inline InferenceResult(const InferenceResult& from) : InferenceResult(nullptr, from) {}
  inline InferenceResult(InferenceResult&& from) noexcept
      : InferenceResult(nullptr, std::move(from)) {}
  inline InferenceResult& operator=(const InferenceResult& from) {
    CopyFrom(from);
    return *this;
  }
  inline InferenceResult& operator=(InferenceResult&& from) noexcept {
    if (this == &from) return *this;
    if (::google::protobuf::internal::CanMoveWithInternalSwap(GetArena(), from.GetArena())) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const
      ABSL_ATTRIBUTE_LIFETIME_BOUND {
    return _internal_metadata_.unknown_fields<::google::protobuf::UnknownFieldSet>(::google::protobuf::UnknownFieldSet::default_instance);
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields()
      ABSL_ATTRIBUTE_LIFETIME_BOUND {
    return _internal_metadata_.mutable_unknown_fields<::google::protobuf::UnknownFieldSet>();
  }

  static const ::google::protobuf::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::google::protobuf::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::google::protobuf::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const InferenceResult& default_instance() {
    return *internal_default_instance();
  }
  enum ResultCase {
    kClassificationResult = 2,
    kRegressionResult = 3,
    RESULT_NOT_SET = 0,
  };
  static inline const InferenceResult* internal_default_instance() {
    return reinterpret_cast<const InferenceResult*>(
        &_InferenceResult_default_instance_);
  }
  static constexpr int kIndexInFileMessages = 1;
  friend void swap(InferenceResult& a, InferenceResult& b) { a.Swap(&b); }
  inline void Swap(InferenceResult* other) {
    if (other == this) return;
    if (::google::protobuf::internal::CanUseInternalSwap(GetArena(), other->GetArena())) {
      InternalSwap(other);
    } else {
      ::google::protobuf::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(InferenceResult* other) {
    if (other == this) return;
    ABSL_DCHECK(GetArena() == other->GetArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  InferenceResult* New(::google::protobuf::Arena* arena = nullptr) const {
    return ::google::protobuf::Message::DefaultConstruct<InferenceResult>(arena);
  }
  using ::google::protobuf::Message::CopyFrom;
  void CopyFrom(const InferenceResult& from);
  using ::google::protobuf::Message::MergeFrom;
  void MergeFrom(const InferenceResult& from) { InferenceResult::MergeImpl(*this, from); }

  private:
  static void MergeImpl(
      ::google::protobuf::MessageLite& to_msg,
      const ::google::protobuf::MessageLite& from_msg);

  public:
  bool IsInitialized() const {
    return true;
  }
  ABSL_ATTRIBUTE_REINITIALIZES void Clear() PROTOBUF_FINAL;
  #if defined(PROTOBUF_CUSTOM_VTABLE)
  private:
  static ::size_t ByteSizeLong(const ::google::protobuf::MessageLite& msg);
  static ::uint8_t* _InternalSerialize(
      const MessageLite& msg, ::uint8_t* target,
      ::google::protobuf::io::EpsCopyOutputStream* stream);

  public:
  ::size_t ByteSizeLong() const { return ByteSizeLong(*this); }
  ::uint8_t* _InternalSerialize(
      ::uint8_t* target,
      ::google::protobuf::io::EpsCopyOutputStream* stream) const {
    return _InternalSerialize(*this, target, stream);
  }
  #else   // PROTOBUF_CUSTOM_VTABLE
  ::size_t ByteSizeLong() const final;
  ::uint8_t* _InternalSerialize(
      ::uint8_t* target,
      ::google::protobuf::io::EpsCopyOutputStream* stream) const final;
  #endif  // PROTOBUF_CUSTOM_VTABLE
  int GetCachedSize() const { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::google::protobuf::Arena* arena);
  static void SharedDtor(MessageLite& self);
  void InternalSwap(InferenceResult* other);
 private:
  template <typename T>
  friend ::absl::string_view(
      ::google::protobuf::internal::GetAnyMessageName)();
  static ::absl::string_view FullMessageName() { return "tensorflow.serving.InferenceResult"; }

 protected:
  explicit InferenceResult(::google::protobuf::Arena* arena);
  InferenceResult(::google::protobuf::Arena* arena, const InferenceResult& from);
  InferenceResult(::google::protobuf::Arena* arena, InferenceResult&& from) noexcept
      : InferenceResult(arena) {
    *this = ::std::move(from);
  }
  const ::google::protobuf::internal::ClassData* GetClassData() const PROTOBUF_FINAL;
  static void* PlacementNew_(const void*, void* mem,
                             ::google::protobuf::Arena* arena);
  static constexpr auto InternalNewImpl_();
  static const ::google::protobuf::internal::ClassDataFull _class_data_;

 public:
  ::google::protobuf::Metadata GetMetadata() const;
  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------
  enum : int {
    kModelSpecFieldNumber = 1,
    kClassificationResultFieldNumber = 2,
    kRegressionResultFieldNumber = 3,
  };
  // .tensorflow.serving.ModelSpec model_spec = 1;
  bool has_model_spec() const;
  void clear_model_spec() ;
  const ::tensorflow::serving::ModelSpec& model_spec() const;
  PROTOBUF_NODISCARD ::tensorflow::serving::ModelSpec* release_model_spec();
  ::tensorflow::serving::ModelSpec* mutable_model_spec();
  void set_allocated_model_spec(::tensorflow::serving::ModelSpec* value);
  void unsafe_arena_set_allocated_model_spec(::tensorflow::serving::ModelSpec* value);
  ::tensorflow::serving::ModelSpec* unsafe_arena_release_model_spec();

  private:
  const ::tensorflow::serving::ModelSpec& _internal_model_spec() const;
  ::tensorflow::serving::ModelSpec* _internal_mutable_model_spec();

  public:
  // .tensorflow.serving.ClassificationResult classification_result = 2;
  bool has_classification_result() const;
  private:
  bool _internal_has_classification_result() const;

  public:
  void clear_classification_result() ;
  const ::tensorflow::serving::ClassificationResult& classification_result() const;
  PROTOBUF_NODISCARD ::tensorflow::serving::ClassificationResult* release_classification_result();
  ::tensorflow::serving::ClassificationResult* mutable_classification_result();
  void set_allocated_classification_result(::tensorflow::serving::ClassificationResult* value);
  void unsafe_arena_set_allocated_classification_result(::tensorflow::serving::ClassificationResult* value);
  ::tensorflow::serving::ClassificationResult* unsafe_arena_release_classification_result();

  private:
  const ::tensorflow::serving::ClassificationResult& _internal_classification_result() const;
  ::tensorflow::serving::ClassificationResult* _internal_mutable_classification_result();

  public:
  // .tensorflow.serving.RegressionResult regression_result = 3;
  bool has_regression_result() const;
  private:
  bool _internal_has_regression_result() const;

  public:
  void clear_regression_result() ;
  const ::tensorflow::serving::RegressionResult& regression_result() const;
  PROTOBUF_NODISCARD ::tensorflow::serving::RegressionResult* release_regression_result();
  ::tensorflow::serving::RegressionResult* mutable_regression_result();
  void set_allocated_regression_result(::tensorflow::serving::RegressionResult* value);
  void unsafe_arena_set_allocated_regression_result(::tensorflow::serving::RegressionResult* value);
  ::tensorflow::serving::RegressionResult* unsafe_arena_release_regression_result();

  private:
  const ::tensorflow::serving::RegressionResult& _internal_regression_result() const;
  ::tensorflow::serving::RegressionResult* _internal_mutable_regression_result();

  public:
  void clear_result();
  ResultCase result_case() const;
  // @@protoc_insertion_point(class_scope:tensorflow.serving.InferenceResult)
 private:
  class _Internal;
  void set_has_classification_result();
  void set_has_regression_result();
  inline bool has_result() const;
  inline void clear_has_result();
  friend class ::google::protobuf::internal::TcParser;
  static const ::google::protobuf::internal::TcParseTable<
      0, 3, 3,
      0, 2>
      _table_;

  friend class ::google::protobuf::MessageLite;
  friend class ::google::protobuf::Arena;
  template <typename T>
  friend class ::google::protobuf::Arena::InternalHelper;
  using InternalArenaConstructable_ = void;
  using DestructorSkippable_ = void;
  struct Impl_ {
    inline explicit constexpr Impl_(
        ::google::protobuf::internal::ConstantInitialized) noexcept;
    inline explicit Impl_(::google::protobuf::internal::InternalVisibility visibility,
                          ::google::protobuf::Arena* arena);
    inline explicit Impl_(::google::protobuf::internal::InternalVisibility visibility,
                          ::google::protobuf::Arena* arena, const Impl_& from,
                          const InferenceResult& from_msg);
    ::google::protobuf::internal::HasBits<1> _has_bits_;
    ::google::protobuf::internal::CachedSize _cached_size_;
    ::tensorflow::serving::ModelSpec* model_spec_;
    union ResultUnion {
      constexpr ResultUnion() : _constinit_{} {}
      ::google::protobuf::internal::ConstantInitialized _constinit_;
      ::tensorflow::serving::ClassificationResult* classification_result_;
      ::tensorflow::serving::RegressionResult* regression_result_;
    } result_;
    ::uint32_t _oneof_case_[1];
    PROTOBUF_TSAN_DECLARE_MEMBER
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_tensorflow_5fserving_2fapis_2finference_2eproto;
};
// -------------------------------------------------------------------

class MultiInferenceResponse final
    : public ::google::protobuf::Message
/* @@protoc_insertion_point(class_definition:tensorflow.serving.MultiInferenceResponse) */ {
 public:
  inline MultiInferenceResponse() : MultiInferenceResponse(nullptr) {}
  ~MultiInferenceResponse() PROTOBUF_FINAL;

#if defined(PROTOBUF_CUSTOM_VTABLE)
  void operator delete(MultiInferenceResponse* msg, std::destroying_delete_t) {
    SharedDtor(*msg);
    ::google::protobuf::internal::SizedDelete(msg, sizeof(MultiInferenceResponse));
  }
#endif

  template <typename = void>
  explicit PROTOBUF_CONSTEXPR MultiInferenceResponse(
      ::google::protobuf::internal::ConstantInitialized);

  inline MultiInferenceResponse(const MultiInferenceResponse& from) : MultiInferenceResponse(nullptr, from) {}
  inline MultiInferenceResponse(MultiInferenceResponse&& from) noexcept
      : MultiInferenceResponse(nullptr, std::move(from)) {}
  inline MultiInferenceResponse& operator=(const MultiInferenceResponse& from) {
    CopyFrom(from);
    return *this;
  }
  inline MultiInferenceResponse& operator=(MultiInferenceResponse&& from) noexcept {
    if (this == &from) return *this;
    if (::google::protobuf::internal::CanMoveWithInternalSwap(GetArena(), from.GetArena())) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const
      ABSL_ATTRIBUTE_LIFETIME_BOUND {
    return _internal_metadata_.unknown_fields<::google::protobuf::UnknownFieldSet>(::google::protobuf::UnknownFieldSet::default_instance);
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields()
      ABSL_ATTRIBUTE_LIFETIME_BOUND {
    return _internal_metadata_.mutable_unknown_fields<::google::protobuf::UnknownFieldSet>();
  }

  static const ::google::protobuf::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::google::protobuf::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::google::protobuf::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const MultiInferenceResponse& default_instance() {
    return *internal_default_instance();
  }
  static inline const MultiInferenceResponse* internal_default_instance() {
    return reinterpret_cast<const MultiInferenceResponse*>(
        &_MultiInferenceResponse_default_instance_);
  }
  static constexpr int kIndexInFileMessages = 3;
  friend void swap(MultiInferenceResponse& a, MultiInferenceResponse& b) { a.Swap(&b); }
  inline void Swap(MultiInferenceResponse* other) {
    if (other == this) return;
    if (::google::protobuf::internal::CanUseInternalSwap(GetArena(), other->GetArena())) {
      InternalSwap(other);
    } else {
      ::google::protobuf::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(MultiInferenceResponse* other) {
    if (other == this) return;
    ABSL_DCHECK(GetArena() == other->GetArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  MultiInferenceResponse* New(::google::protobuf::Arena* arena = nullptr) const {
    return ::google::protobuf::Message::DefaultConstruct<MultiInferenceResponse>(arena);
  }
  using ::google::protobuf::Message::CopyFrom;
  void CopyFrom(const MultiInferenceResponse& from);
  using ::google::protobuf::Message::MergeFrom;
  void MergeFrom(const MultiInferenceResponse& from) { MultiInferenceResponse::MergeImpl(*this, from); }

  private:
  static void MergeImpl(
      ::google::protobuf::MessageLite& to_msg,
      const ::google::protobuf::MessageLite& from_msg);

  public:
  bool IsInitialized() const {
    return true;
  }
  ABSL_ATTRIBUTE_REINITIALIZES void Clear() PROTOBUF_FINAL;
  #if defined(PROTOBUF_CUSTOM_VTABLE)
  private:
  static ::size_t ByteSizeLong(const ::google::protobuf::MessageLite& msg);
  static ::uint8_t* _InternalSerialize(
      const MessageLite& msg, ::uint8_t* target,
      ::google::protobuf::io::EpsCopyOutputStream* stream);

  public:
  ::size_t ByteSizeLong() const { return ByteSizeLong(*this); }
  ::uint8_t* _InternalSerialize(
      ::uint8_t* target,
      ::google::protobuf::io::EpsCopyOutputStream* stream) const {
    return _InternalSerialize(*this, target, stream);
  }
  #else   // PROTOBUF_CUSTOM_VTABLE
  ::size_t ByteSizeLong() const final;
  ::uint8_t* _InternalSerialize(
      ::uint8_t* target,
      ::google::protobuf::io::EpsCopyOutputStream* stream) const final;
  #endif  // PROTOBUF_CUSTOM_VTABLE
  int GetCachedSize() const { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::google::protobuf::Arena* arena);
  static void SharedDtor(MessageLite& self);
  void InternalSwap(MultiInferenceResponse* other);
 private:
  template <typename T>
  friend ::absl::string_view(
      ::google::protobuf::internal::GetAnyMessageName)();
  static ::absl::string_view FullMessageName() { return "tensorflow.serving.MultiInferenceResponse"; }

 protected:
  explicit MultiInferenceResponse(::google::protobuf::Arena* arena);
  MultiInferenceResponse(::google::protobuf::Arena* arena, const MultiInferenceResponse& from);
  MultiInferenceResponse(::google::protobuf::Arena* arena, MultiInferenceResponse&& from) noexcept
      : MultiInferenceResponse(arena) {
    *this = ::std::move(from);
  }
  const ::google::protobuf::internal::ClassData* GetClassData() const PROTOBUF_FINAL;
  static void* PlacementNew_(const void*, void* mem,
                             ::google::protobuf::Arena* arena);
  static constexpr auto InternalNewImpl_();
  static const ::google::protobuf::internal::ClassDataFull _class_data_;

 public:
  ::google::protobuf::Metadata GetMetadata() const;
  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------
  enum : int {
    kResultsFieldNumber = 1,
  };
  // repeated .tensorflow.serving.InferenceResult results = 1;
  int results_size() const;
  private:
  int _internal_results_size() const;

  public:
  void clear_results() ;
  ::tensorflow::serving::InferenceResult* mutable_results(int index);
  ::google::protobuf::RepeatedPtrField<::tensorflow::serving::InferenceResult>* mutable_results();

  private:
  const ::google::protobuf::RepeatedPtrField<::tensorflow::serving::InferenceResult>& _internal_results() const;
  ::google::protobuf::RepeatedPtrField<::tensorflow::serving::InferenceResult>* _internal_mutable_results();
  public:
  const ::tensorflow::serving::InferenceResult& results(int index) const;
  ::tensorflow::serving::InferenceResult* add_results();
  const ::google::protobuf::RepeatedPtrField<::tensorflow::serving::InferenceResult>& results() const;
  // @@protoc_insertion_point(class_scope:tensorflow.serving.MultiInferenceResponse)
 private:
  class _Internal;
  friend class ::google::protobuf::internal::TcParser;
  static const ::google::protobuf::internal::TcParseTable<
      0, 1, 1,
      0, 2>
      _table_;

  friend class ::google::protobuf::MessageLite;
  friend class ::google::protobuf::Arena;
  template <typename T>
  friend class ::google::protobuf::Arena::InternalHelper;
  using InternalArenaConstructable_ = void;
  using DestructorSkippable_ = void;
  struct Impl_ {
    inline explicit constexpr Impl_(
        ::google::protobuf::internal::ConstantInitialized) noexcept;
    inline explicit Impl_(::google::protobuf::internal::InternalVisibility visibility,
                          ::google::protobuf::Arena* arena);
    inline explicit Impl_(::google::protobuf::internal::InternalVisibility visibility,
                          ::google::protobuf::Arena* arena, const Impl_& from,
                          const MultiInferenceResponse& from_msg);
    ::google::protobuf::RepeatedPtrField< ::tensorflow::serving::InferenceResult > results_;
    ::google::protobuf::internal::CachedSize _cached_size_;
    PROTOBUF_TSAN_DECLARE_MEMBER
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_tensorflow_5fserving_2fapis_2finference_2eproto;
};
// -------------------------------------------------------------------

class MultiInferenceRequest final
    : public ::google::protobuf::Message
/* @@protoc_insertion_point(class_definition:tensorflow.serving.MultiInferenceRequest) */ {
 public:
  inline MultiInferenceRequest() : MultiInferenceRequest(nullptr) {}
  ~MultiInferenceRequest() PROTOBUF_FINAL;

#if defined(PROTOBUF_CUSTOM_VTABLE)
  void operator delete(MultiInferenceRequest* msg, std::destroying_delete_t) {
    SharedDtor(*msg);
    ::google::protobuf::internal::SizedDelete(msg, sizeof(MultiInferenceRequest));
  }
#endif

  template <typename = void>
  explicit PROTOBUF_CONSTEXPR MultiInferenceRequest(
      ::google::protobuf::internal::ConstantInitialized);

  inline MultiInferenceRequest(const MultiInferenceRequest& from) : MultiInferenceRequest(nullptr, from) {}
  inline MultiInferenceRequest(MultiInferenceRequest&& from) noexcept
      : MultiInferenceRequest(nullptr, std::move(from)) {}
  inline MultiInferenceRequest& operator=(const MultiInferenceRequest& from) {
    CopyFrom(from);
    return *this;
  }
  inline MultiInferenceRequest& operator=(MultiInferenceRequest&& from) noexcept {
    if (this == &from) return *this;
    if (::google::protobuf::internal::CanMoveWithInternalSwap(GetArena(), from.GetArena())) {
      InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }

  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const
      ABSL_ATTRIBUTE_LIFETIME_BOUND {
    return _internal_metadata_.unknown_fields<::google::protobuf::UnknownFieldSet>(::google::protobuf::UnknownFieldSet::default_instance);
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields()
      ABSL_ATTRIBUTE_LIFETIME_BOUND {
    return _internal_metadata_.mutable_unknown_fields<::google::protobuf::UnknownFieldSet>();
  }

  static const ::google::protobuf::Descriptor* descriptor() {
    return GetDescriptor();
  }
  static const ::google::protobuf::Descriptor* GetDescriptor() {
    return default_instance().GetMetadata().descriptor;
  }
  static const ::google::protobuf::Reflection* GetReflection() {
    return default_instance().GetMetadata().reflection;
  }
  static const MultiInferenceRequest& default_instance() {
    return *internal_default_instance();
  }
  static inline const MultiInferenceRequest* internal_default_instance() {
    return reinterpret_cast<const MultiInferenceRequest*>(
        &_MultiInferenceRequest_default_instance_);
  }
  static constexpr int kIndexInFileMessages = 2;
  friend void swap(MultiInferenceRequest& a, MultiInferenceRequest& b) { a.Swap(&b); }
  inline void Swap(MultiInferenceRequest* other) {
    if (other == this) return;
    if (::google::protobuf::internal::CanUseInternalSwap(GetArena(), other->GetArena())) {
      InternalSwap(other);
    } else {
      ::google::protobuf::internal::GenericSwap(this, other);
    }
  }
  void UnsafeArenaSwap(MultiInferenceRequest* other) {
    if (other == this) return;
    ABSL_DCHECK(GetArena() == other->GetArena());
    InternalSwap(other);
  }

  // implements Message ----------------------------------------------

  MultiInferenceRequest* New(::google::protobuf::Arena* arena = nullptr) const {
    return ::google::protobuf::Message::DefaultConstruct<MultiInferenceRequest>(arena);
  }
  using ::google::protobuf::Message::CopyFrom;
  void CopyFrom(const MultiInferenceRequest& from);
  using ::google::protobuf::Message::MergeFrom;
  void MergeFrom(const MultiInferenceRequest& from) { MultiInferenceRequest::MergeImpl(*this, from); }

  private:
  static void MergeImpl(
      ::google::protobuf::MessageLite& to_msg,
      const ::google::protobuf::MessageLite& from_msg);

  public:
  bool IsInitialized() const {
    return true;
  }
  ABSL_ATTRIBUTE_REINITIALIZES void Clear() PROTOBUF_FINAL;
  #if defined(PROTOBUF_CUSTOM_VTABLE)
  private:
  static ::size_t ByteSizeLong(const ::google::protobuf::MessageLite& msg);
  static ::uint8_t* _InternalSerialize(
      const MessageLite& msg, ::uint8_t* target,
      ::google::protobuf::io::EpsCopyOutputStream* stream);

  public:
  ::size_t ByteSizeLong() const { return ByteSizeLong(*this); }
  ::uint8_t* _InternalSerialize(
      ::uint8_t* target,
      ::google::protobuf::io::EpsCopyOutputStream* stream) const {
    return _InternalSerialize(*this, target, stream);
  }
  #else   // PROTOBUF_CUSTOM_VTABLE
  ::size_t ByteSizeLong() const final;
  ::uint8_t* _InternalSerialize(
      ::uint8_t* target,
      ::google::protobuf::io::EpsCopyOutputStream* stream) const final;
  #endif  // PROTOBUF_CUSTOM_VTABLE
  int GetCachedSize() const { return _impl_._cached_size_.Get(); }

  private:
  void SharedCtor(::google::protobuf::Arena* arena);
  static void SharedDtor(MessageLite& self);
  void InternalSwap(MultiInferenceRequest* other);
 private:
  template <typename T>
  friend ::absl::string_view(
      ::google::protobuf::internal::GetAnyMessageName)();
  static ::absl::string_view FullMessageName() { return "tensorflow.serving.MultiInferenceRequest"; }

 protected:
  explicit MultiInferenceRequest(::google::protobuf::Arena* arena);
  MultiInferenceRequest(::google::protobuf::Arena* arena, const MultiInferenceRequest& from);
  MultiInferenceRequest(::google::protobuf::Arena* arena, MultiInferenceRequest&& from) noexcept
      : MultiInferenceRequest(arena) {
    *this = ::std::move(from);
  }
  const ::google::protobuf::internal::ClassData* GetClassData() const PROTOBUF_FINAL;
  static void* PlacementNew_(const void*, void* mem,
                             ::google::protobuf::Arena* arena);
  static constexpr auto InternalNewImpl_();
  static const ::google::protobuf::internal::ClassDataFull _class_data_;

 public:
  ::google::protobuf::Metadata GetMetadata() const;
  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------
  enum : int {
    kTasksFieldNumber = 1,
    kInputFieldNumber = 2,
  };
  // repeated .tensorflow.serving.InferenceTask tasks = 1;
  int tasks_size() const;
  private:
  int _internal_tasks_size() const;

  public:
  void clear_tasks() ;
  ::tensorflow::serving::InferenceTask* mutable_tasks(int index);
  ::google::protobuf::RepeatedPtrField<::tensorflow::serving::InferenceTask>* mutable_tasks();

  private:
  const ::google::protobuf::RepeatedPtrField<::tensorflow::serving::InferenceTask>& _internal_tasks() const;
  ::google::protobuf::RepeatedPtrField<::tensorflow::serving::InferenceTask>* _internal_mutable_tasks();
  public:
  const ::tensorflow::serving::InferenceTask& tasks(int index) const;
  ::tensorflow::serving::InferenceTask* add_tasks();
  const ::google::protobuf::RepeatedPtrField<::tensorflow::serving::InferenceTask>& tasks() const;
  // .tensorflow.serving.Input input = 2;
  bool has_input() const;
  void clear_input() ;
  const ::tensorflow::serving::Input& input() const;
  PROTOBUF_NODISCARD ::tensorflow::serving::Input* release_input();
  ::tensorflow::serving::Input* mutable_input();
  void set_allocated_input(::tensorflow::serving::Input* value);
  void unsafe_arena_set_allocated_input(::tensorflow::serving::Input* value);
  ::tensorflow::serving::Input* unsafe_arena_release_input();

  private:
  const ::tensorflow::serving::Input& _internal_input() const;
  ::tensorflow::serving::Input* _internal_mutable_input();

  public:
  // @@protoc_insertion_point(class_scope:tensorflow.serving.MultiInferenceRequest)
 private:
  class _Internal;
  friend class ::google::protobuf::internal::TcParser;
  static const ::google::protobuf::internal::TcParseTable<
      1, 2, 2,
      0, 2>
      _table_;

  friend class ::google::protobuf::MessageLite;
  friend class ::google::protobuf::Arena;
  template <typename T>
  friend class ::google::protobuf::Arena::InternalHelper;
  using InternalArenaConstructable_ = void;
  using DestructorSkippable_ = void;
  struct Impl_ {
    inline explicit constexpr Impl_(
        ::google::protobuf::internal::ConstantInitialized) noexcept;
    inline explicit Impl_(::google::protobuf::internal::InternalVisibility visibility,
                          ::google::protobuf::Arena* arena);
    inline explicit Impl_(::google::protobuf::internal::InternalVisibility visibility,
                          ::google::protobuf::Arena* arena, const Impl_& from,
                          const MultiInferenceRequest& from_msg);
    ::google::protobuf::internal::HasBits<1> _has_bits_;
    ::google::protobuf::internal::CachedSize _cached_size_;
    ::google::protobuf::RepeatedPtrField< ::tensorflow::serving::InferenceTask > tasks_;
    ::tensorflow::serving::Input* input_;
    PROTOBUF_TSAN_DECLARE_MEMBER
  };
  union { Impl_ _impl_; };
  friend struct ::TableStruct_tensorflow_5fserving_2fapis_2finference_2eproto;
};

// ===================================================================




// ===================================================================


#ifdef __GNUC__
#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wstrict-aliasing"
#endif  // __GNUC__
// -------------------------------------------------------------------

// InferenceTask

// .tensorflow.serving.ModelSpec model_spec = 1;
inline bool InferenceTask::has_model_spec() const {
  bool value = (_impl_._has_bits_[0] & 0x00000001u) != 0;
  PROTOBUF_ASSUME(!value || _impl_.model_spec_ != nullptr);
  return value;
}
inline const ::tensorflow::serving::ModelSpec& InferenceTask::_internal_model_spec() const {
  ::google::protobuf::internal::TSanRead(&_impl_);
  const ::tensorflow::serving::ModelSpec* p = _impl_.model_spec_;
  return p != nullptr ? *p : reinterpret_cast<const ::tensorflow::serving::ModelSpec&>(::tensorflow::serving::_ModelSpec_default_instance_);
}
inline const ::tensorflow::serving::ModelSpec& InferenceTask::model_spec() const ABSL_ATTRIBUTE_LIFETIME_BOUND {
  // @@protoc_insertion_point(field_get:tensorflow.serving.InferenceTask.model_spec)
  return _internal_model_spec();
}
inline void InferenceTask::unsafe_arena_set_allocated_model_spec(::tensorflow::serving::ModelSpec* value) {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  if (GetArena() == nullptr) {
    delete reinterpret_cast<::google::protobuf::MessageLite*>(_impl_.model_spec_);
  }
  _impl_.model_spec_ = reinterpret_cast<::tensorflow::serving::ModelSpec*>(value);
  if (value != nullptr) {
    _impl_._has_bits_[0] |= 0x00000001u;
  } else {
    _impl_._has_bits_[0] &= ~0x00000001u;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.InferenceTask.model_spec)
}
inline ::tensorflow::serving::ModelSpec* InferenceTask::release_model_spec() {
  ::google::protobuf::internal::TSanWrite(&_impl_);

  _impl_._has_bits_[0] &= ~0x00000001u;
  ::tensorflow::serving::ModelSpec* released = _impl_.model_spec_;
  _impl_.model_spec_ = nullptr;
  if (::google::protobuf::internal::DebugHardenForceCopyInRelease()) {
    auto* old = reinterpret_cast<::google::protobuf::MessageLite*>(released);
    released = ::google::protobuf::internal::DuplicateIfNonNull(released);
    if (GetArena() == nullptr) {
      delete old;
    }
  } else {
    if (GetArena() != nullptr) {
      released = ::google::protobuf::internal::DuplicateIfNonNull(released);
    }
  }
  return released;
}
inline ::tensorflow::serving::ModelSpec* InferenceTask::unsafe_arena_release_model_spec() {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  // @@protoc_insertion_point(field_release:tensorflow.serving.InferenceTask.model_spec)

  _impl_._has_bits_[0] &= ~0x00000001u;
  ::tensorflow::serving::ModelSpec* temp = _impl_.model_spec_;
  _impl_.model_spec_ = nullptr;
  return temp;
}
inline ::tensorflow::serving::ModelSpec* InferenceTask::_internal_mutable_model_spec() {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  if (_impl_.model_spec_ == nullptr) {
    auto* p = ::google::protobuf::Message::DefaultConstruct<::tensorflow::serving::ModelSpec>(GetArena());
    _impl_.model_spec_ = reinterpret_cast<::tensorflow::serving::ModelSpec*>(p);
  }
  return _impl_.model_spec_;
}
inline ::tensorflow::serving::ModelSpec* InferenceTask::mutable_model_spec() ABSL_ATTRIBUTE_LIFETIME_BOUND {
  _impl_._has_bits_[0] |= 0x00000001u;
  ::tensorflow::serving::ModelSpec* _msg = _internal_mutable_model_spec();
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.InferenceTask.model_spec)
  return _msg;
}
inline void InferenceTask::set_allocated_model_spec(::tensorflow::serving::ModelSpec* value) {
  ::google::protobuf::Arena* message_arena = GetArena();
  ::google::protobuf::internal::TSanWrite(&_impl_);
  if (message_arena == nullptr) {
    delete reinterpret_cast<::google::protobuf::MessageLite*>(_impl_.model_spec_);
  }

  if (value != nullptr) {
    ::google::protobuf::Arena* submessage_arena = reinterpret_cast<::google::protobuf::MessageLite*>(value)->GetArena();
    if (message_arena != submessage_arena) {
      value = ::google::protobuf::internal::GetOwnedMessage(message_arena, value, submessage_arena);
    }
    _impl_._has_bits_[0] |= 0x00000001u;
  } else {
    _impl_._has_bits_[0] &= ~0x00000001u;
  }

  _impl_.model_spec_ = reinterpret_cast<::tensorflow::serving::ModelSpec*>(value);
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.InferenceTask.model_spec)
}

// string method_name = 2;
inline void InferenceTask::clear_method_name() {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  _impl_.method_name_.ClearToEmpty();
}
inline const std::string& InferenceTask::method_name() const
    ABSL_ATTRIBUTE_LIFETIME_BOUND {
  // @@protoc_insertion_point(field_get:tensorflow.serving.InferenceTask.method_name)
  return _internal_method_name();
}
template <typename Arg_, typename... Args_>
inline PROTOBUF_ALWAYS_INLINE void InferenceTask::set_method_name(Arg_&& arg,
                                                     Args_... args) {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  _impl_.method_name_.Set(static_cast<Arg_&&>(arg), args..., GetArena());
  // @@protoc_insertion_point(field_set:tensorflow.serving.InferenceTask.method_name)
}
inline std::string* InferenceTask::mutable_method_name() ABSL_ATTRIBUTE_LIFETIME_BOUND {
  std::string* _s = _internal_mutable_method_name();
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.InferenceTask.method_name)
  return _s;
}
inline const std::string& InferenceTask::_internal_method_name() const {
  ::google::protobuf::internal::TSanRead(&_impl_);
  return _impl_.method_name_.Get();
}
inline void InferenceTask::_internal_set_method_name(const std::string& value) {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  _impl_.method_name_.Set(value, GetArena());
}
inline std::string* InferenceTask::_internal_mutable_method_name() {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  return _impl_.method_name_.Mutable( GetArena());
}
inline std::string* InferenceTask::release_method_name() {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  // @@protoc_insertion_point(field_release:tensorflow.serving.InferenceTask.method_name)
  return _impl_.method_name_.Release();
}
inline void InferenceTask::set_allocated_method_name(std::string* value) {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  _impl_.method_name_.SetAllocated(value, GetArena());
  if (::google::protobuf::internal::DebugHardenForceCopyDefaultString() && _impl_.method_name_.IsDefault()) {
    _impl_.method_name_.Set("", GetArena());
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.InferenceTask.method_name)
}

// -------------------------------------------------------------------

// InferenceResult

// .tensorflow.serving.ModelSpec model_spec = 1;
inline bool InferenceResult::has_model_spec() const {
  bool value = (_impl_._has_bits_[0] & 0x00000001u) != 0;
  PROTOBUF_ASSUME(!value || _impl_.model_spec_ != nullptr);
  return value;
}
inline const ::tensorflow::serving::ModelSpec& InferenceResult::_internal_model_spec() const {
  ::google::protobuf::internal::TSanRead(&_impl_);
  const ::tensorflow::serving::ModelSpec* p = _impl_.model_spec_;
  return p != nullptr ? *p : reinterpret_cast<const ::tensorflow::serving::ModelSpec&>(::tensorflow::serving::_ModelSpec_default_instance_);
}
inline const ::tensorflow::serving::ModelSpec& InferenceResult::model_spec() const ABSL_ATTRIBUTE_LIFETIME_BOUND {
  // @@protoc_insertion_point(field_get:tensorflow.serving.InferenceResult.model_spec)
  return _internal_model_spec();
}
inline void InferenceResult::unsafe_arena_set_allocated_model_spec(::tensorflow::serving::ModelSpec* value) {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  if (GetArena() == nullptr) {
    delete reinterpret_cast<::google::protobuf::MessageLite*>(_impl_.model_spec_);
  }
  _impl_.model_spec_ = reinterpret_cast<::tensorflow::serving::ModelSpec*>(value);
  if (value != nullptr) {
    _impl_._has_bits_[0] |= 0x00000001u;
  } else {
    _impl_._has_bits_[0] &= ~0x00000001u;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.InferenceResult.model_spec)
}
inline ::tensorflow::serving::ModelSpec* InferenceResult::release_model_spec() {
  ::google::protobuf::internal::TSanWrite(&_impl_);

  _impl_._has_bits_[0] &= ~0x00000001u;
  ::tensorflow::serving::ModelSpec* released = _impl_.model_spec_;
  _impl_.model_spec_ = nullptr;
  if (::google::protobuf::internal::DebugHardenForceCopyInRelease()) {
    auto* old = reinterpret_cast<::google::protobuf::MessageLite*>(released);
    released = ::google::protobuf::internal::DuplicateIfNonNull(released);
    if (GetArena() == nullptr) {
      delete old;
    }
  } else {
    if (GetArena() != nullptr) {
      released = ::google::protobuf::internal::DuplicateIfNonNull(released);
    }
  }
  return released;
}
inline ::tensorflow::serving::ModelSpec* InferenceResult::unsafe_arena_release_model_spec() {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  // @@protoc_insertion_point(field_release:tensorflow.serving.InferenceResult.model_spec)

  _impl_._has_bits_[0] &= ~0x00000001u;
  ::tensorflow::serving::ModelSpec* temp = _impl_.model_spec_;
  _impl_.model_spec_ = nullptr;
  return temp;
}
inline ::tensorflow::serving::ModelSpec* InferenceResult::_internal_mutable_model_spec() {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  if (_impl_.model_spec_ == nullptr) {
    auto* p = ::google::protobuf::Message::DefaultConstruct<::tensorflow::serving::ModelSpec>(GetArena());
    _impl_.model_spec_ = reinterpret_cast<::tensorflow::serving::ModelSpec*>(p);
  }
  return _impl_.model_spec_;
}
inline ::tensorflow::serving::ModelSpec* InferenceResult::mutable_model_spec() ABSL_ATTRIBUTE_LIFETIME_BOUND {
  _impl_._has_bits_[0] |= 0x00000001u;
  ::tensorflow::serving::ModelSpec* _msg = _internal_mutable_model_spec();
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.InferenceResult.model_spec)
  return _msg;
}
inline void InferenceResult::set_allocated_model_spec(::tensorflow::serving::ModelSpec* value) {
  ::google::protobuf::Arena* message_arena = GetArena();
  ::google::protobuf::internal::TSanWrite(&_impl_);
  if (message_arena == nullptr) {
    delete reinterpret_cast<::google::protobuf::MessageLite*>(_impl_.model_spec_);
  }

  if (value != nullptr) {
    ::google::protobuf::Arena* submessage_arena = reinterpret_cast<::google::protobuf::MessageLite*>(value)->GetArena();
    if (message_arena != submessage_arena) {
      value = ::google::protobuf::internal::GetOwnedMessage(message_arena, value, submessage_arena);
    }
    _impl_._has_bits_[0] |= 0x00000001u;
  } else {
    _impl_._has_bits_[0] &= ~0x00000001u;
  }

  _impl_.model_spec_ = reinterpret_cast<::tensorflow::serving::ModelSpec*>(value);
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.InferenceResult.model_spec)
}

// .tensorflow.serving.ClassificationResult classification_result = 2;
inline bool InferenceResult::has_classification_result() const {
  return result_case() == kClassificationResult;
}
inline bool InferenceResult::_internal_has_classification_result() const {
  return result_case() == kClassificationResult;
}
inline void InferenceResult::set_has_classification_result() {
  _impl_._oneof_case_[0] = kClassificationResult;
}
inline ::tensorflow::serving::ClassificationResult* InferenceResult::release_classification_result() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.InferenceResult.classification_result)
  if (result_case() == kClassificationResult) {
    clear_has_result();
    auto* temp = _impl_.result_.classification_result_;
    if (GetArena() != nullptr) {
      temp = ::google::protobuf::internal::DuplicateIfNonNull(temp);
    }
    _impl_.result_.classification_result_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::tensorflow::serving::ClassificationResult& InferenceResult::_internal_classification_result() const {
  return result_case() == kClassificationResult ? *_impl_.result_.classification_result_ : reinterpret_cast<::tensorflow::serving::ClassificationResult&>(::tensorflow::serving::_ClassificationResult_default_instance_);
}
inline const ::tensorflow::serving::ClassificationResult& InferenceResult::classification_result() const ABSL_ATTRIBUTE_LIFETIME_BOUND {
  // @@protoc_insertion_point(field_get:tensorflow.serving.InferenceResult.classification_result)
  return _internal_classification_result();
}
inline ::tensorflow::serving::ClassificationResult* InferenceResult::unsafe_arena_release_classification_result() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.InferenceResult.classification_result)
  if (result_case() == kClassificationResult) {
    clear_has_result();
    auto* temp = _impl_.result_.classification_result_;
    _impl_.result_.classification_result_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void InferenceResult::unsafe_arena_set_allocated_classification_result(::tensorflow::serving::ClassificationResult* value) {
  // We rely on the oneof clear method to free the earlier contents
  // of this oneof. We can directly use the pointer we're given to
  // set the new value.
  clear_result();
  if (value) {
    set_has_classification_result();
    _impl_.result_.classification_result_ = value;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.InferenceResult.classification_result)
}
inline ::tensorflow::serving::ClassificationResult* InferenceResult::_internal_mutable_classification_result() {
  if (result_case() != kClassificationResult) {
    clear_result();
    set_has_classification_result();
    _impl_.result_.classification_result_ =
        ::google::protobuf::Message::DefaultConstruct<::tensorflow::serving::ClassificationResult>(GetArena());
  }
  return _impl_.result_.classification_result_;
}
inline ::tensorflow::serving::ClassificationResult* InferenceResult::mutable_classification_result() ABSL_ATTRIBUTE_LIFETIME_BOUND {
  ::tensorflow::serving::ClassificationResult* _msg = _internal_mutable_classification_result();
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.InferenceResult.classification_result)
  return _msg;
}

// .tensorflow.serving.RegressionResult regression_result = 3;
inline bool InferenceResult::has_regression_result() const {
  return result_case() == kRegressionResult;
}
inline bool InferenceResult::_internal_has_regression_result() const {
  return result_case() == kRegressionResult;
}
inline void InferenceResult::set_has_regression_result() {
  _impl_._oneof_case_[0] = kRegressionResult;
}
inline ::tensorflow::serving::RegressionResult* InferenceResult::release_regression_result() {
  // @@protoc_insertion_point(field_release:tensorflow.serving.InferenceResult.regression_result)
  if (result_case() == kRegressionResult) {
    clear_has_result();
    auto* temp = _impl_.result_.regression_result_;
    if (GetArena() != nullptr) {
      temp = ::google::protobuf::internal::DuplicateIfNonNull(temp);
    }
    _impl_.result_.regression_result_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline const ::tensorflow::serving::RegressionResult& InferenceResult::_internal_regression_result() const {
  return result_case() == kRegressionResult ? *_impl_.result_.regression_result_ : reinterpret_cast<::tensorflow::serving::RegressionResult&>(::tensorflow::serving::_RegressionResult_default_instance_);
}
inline const ::tensorflow::serving::RegressionResult& InferenceResult::regression_result() const ABSL_ATTRIBUTE_LIFETIME_BOUND {
  // @@protoc_insertion_point(field_get:tensorflow.serving.InferenceResult.regression_result)
  return _internal_regression_result();
}
inline ::tensorflow::serving::RegressionResult* InferenceResult::unsafe_arena_release_regression_result() {
  // @@protoc_insertion_point(field_unsafe_arena_release:tensorflow.serving.InferenceResult.regression_result)
  if (result_case() == kRegressionResult) {
    clear_has_result();
    auto* temp = _impl_.result_.regression_result_;
    _impl_.result_.regression_result_ = nullptr;
    return temp;
  } else {
    return nullptr;
  }
}
inline void InferenceResult::unsafe_arena_set_allocated_regression_result(::tensorflow::serving::RegressionResult* value) {
  // We rely on the oneof clear method to free the earlier contents
  // of this oneof. We can directly use the pointer we're given to
  // set the new value.
  clear_result();
  if (value) {
    set_has_regression_result();
    _impl_.result_.regression_result_ = value;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.InferenceResult.regression_result)
}
inline ::tensorflow::serving::RegressionResult* InferenceResult::_internal_mutable_regression_result() {
  if (result_case() != kRegressionResult) {
    clear_result();
    set_has_regression_result();
    _impl_.result_.regression_result_ =
        ::google::protobuf::Message::DefaultConstruct<::tensorflow::serving::RegressionResult>(GetArena());
  }
  return _impl_.result_.regression_result_;
}
inline ::tensorflow::serving::RegressionResult* InferenceResult::mutable_regression_result() ABSL_ATTRIBUTE_LIFETIME_BOUND {
  ::tensorflow::serving::RegressionResult* _msg = _internal_mutable_regression_result();
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.InferenceResult.regression_result)
  return _msg;
}

inline bool InferenceResult::has_result() const {
  return result_case() != RESULT_NOT_SET;
}
inline void InferenceResult::clear_has_result() {
  _impl_._oneof_case_[0] = RESULT_NOT_SET;
}
inline InferenceResult::ResultCase InferenceResult::result_case() const {
  return InferenceResult::ResultCase(_impl_._oneof_case_[0]);
}
// -------------------------------------------------------------------

// MultiInferenceRequest

// repeated .tensorflow.serving.InferenceTask tasks = 1;
inline int MultiInferenceRequest::_internal_tasks_size() const {
  return _internal_tasks().size();
}
inline int MultiInferenceRequest::tasks_size() const {
  return _internal_tasks_size();
}
inline void MultiInferenceRequest::clear_tasks() {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  _impl_.tasks_.Clear();
}
inline ::tensorflow::serving::InferenceTask* MultiInferenceRequest::mutable_tasks(int index)
    ABSL_ATTRIBUTE_LIFETIME_BOUND {
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.MultiInferenceRequest.tasks)
  return _internal_mutable_tasks()->Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField<::tensorflow::serving::InferenceTask>* MultiInferenceRequest::mutable_tasks()
    ABSL_ATTRIBUTE_LIFETIME_BOUND {
  // @@protoc_insertion_point(field_mutable_list:tensorflow.serving.MultiInferenceRequest.tasks)
  ::google::protobuf::internal::TSanWrite(&_impl_);
  return _internal_mutable_tasks();
}
inline const ::tensorflow::serving::InferenceTask& MultiInferenceRequest::tasks(int index) const
    ABSL_ATTRIBUTE_LIFETIME_BOUND {
  // @@protoc_insertion_point(field_get:tensorflow.serving.MultiInferenceRequest.tasks)
  return _internal_tasks().Get(index);
}
inline ::tensorflow::serving::InferenceTask* MultiInferenceRequest::add_tasks() ABSL_ATTRIBUTE_LIFETIME_BOUND {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  ::tensorflow::serving::InferenceTask* _add = _internal_mutable_tasks()->Add();
  // @@protoc_insertion_point(field_add:tensorflow.serving.MultiInferenceRequest.tasks)
  return _add;
}
inline const ::google::protobuf::RepeatedPtrField<::tensorflow::serving::InferenceTask>& MultiInferenceRequest::tasks() const
    ABSL_ATTRIBUTE_LIFETIME_BOUND {
  // @@protoc_insertion_point(field_list:tensorflow.serving.MultiInferenceRequest.tasks)
  return _internal_tasks();
}
inline const ::google::protobuf::RepeatedPtrField<::tensorflow::serving::InferenceTask>&
MultiInferenceRequest::_internal_tasks() const {
  ::google::protobuf::internal::TSanRead(&_impl_);
  return _impl_.tasks_;
}
inline ::google::protobuf::RepeatedPtrField<::tensorflow::serving::InferenceTask>*
MultiInferenceRequest::_internal_mutable_tasks() {
  ::google::protobuf::internal::TSanRead(&_impl_);
  return &_impl_.tasks_;
}

// .tensorflow.serving.Input input = 2;
inline bool MultiInferenceRequest::has_input() const {
  bool value = (_impl_._has_bits_[0] & 0x00000001u) != 0;
  PROTOBUF_ASSUME(!value || _impl_.input_ != nullptr);
  return value;
}
inline const ::tensorflow::serving::Input& MultiInferenceRequest::_internal_input() const {
  ::google::protobuf::internal::TSanRead(&_impl_);
  const ::tensorflow::serving::Input* p = _impl_.input_;
  return p != nullptr ? *p : reinterpret_cast<const ::tensorflow::serving::Input&>(::tensorflow::serving::_Input_default_instance_);
}
inline const ::tensorflow::serving::Input& MultiInferenceRequest::input() const ABSL_ATTRIBUTE_LIFETIME_BOUND {
  // @@protoc_insertion_point(field_get:tensorflow.serving.MultiInferenceRequest.input)
  return _internal_input();
}
inline void MultiInferenceRequest::unsafe_arena_set_allocated_input(::tensorflow::serving::Input* value) {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  if (GetArena() == nullptr) {
    delete reinterpret_cast<::google::protobuf::MessageLite*>(_impl_.input_);
  }
  _impl_.input_ = reinterpret_cast<::tensorflow::serving::Input*>(value);
  if (value != nullptr) {
    _impl_._has_bits_[0] |= 0x00000001u;
  } else {
    _impl_._has_bits_[0] &= ~0x00000001u;
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.MultiInferenceRequest.input)
}
inline ::tensorflow::serving::Input* MultiInferenceRequest::release_input() {
  ::google::protobuf::internal::TSanWrite(&_impl_);

  _impl_._has_bits_[0] &= ~0x00000001u;
  ::tensorflow::serving::Input* released = _impl_.input_;
  _impl_.input_ = nullptr;
  if (::google::protobuf::internal::DebugHardenForceCopyInRelease()) {
    auto* old = reinterpret_cast<::google::protobuf::MessageLite*>(released);
    released = ::google::protobuf::internal::DuplicateIfNonNull(released);
    if (GetArena() == nullptr) {
      delete old;
    }
  } else {
    if (GetArena() != nullptr) {
      released = ::google::protobuf::internal::DuplicateIfNonNull(released);
    }
  }
  return released;
}
inline ::tensorflow::serving::Input* MultiInferenceRequest::unsafe_arena_release_input() {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  // @@protoc_insertion_point(field_release:tensorflow.serving.MultiInferenceRequest.input)

  _impl_._has_bits_[0] &= ~0x00000001u;
  ::tensorflow::serving::Input* temp = _impl_.input_;
  _impl_.input_ = nullptr;
  return temp;
}
inline ::tensorflow::serving::Input* MultiInferenceRequest::_internal_mutable_input() {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  if (_impl_.input_ == nullptr) {
    auto* p = ::google::protobuf::Message::DefaultConstruct<::tensorflow::serving::Input>(GetArena());
    _impl_.input_ = reinterpret_cast<::tensorflow::serving::Input*>(p);
  }
  return _impl_.input_;
}
inline ::tensorflow::serving::Input* MultiInferenceRequest::mutable_input() ABSL_ATTRIBUTE_LIFETIME_BOUND {
  _impl_._has_bits_[0] |= 0x00000001u;
  ::tensorflow::serving::Input* _msg = _internal_mutable_input();
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.MultiInferenceRequest.input)
  return _msg;
}
inline void MultiInferenceRequest::set_allocated_input(::tensorflow::serving::Input* value) {
  ::google::protobuf::Arena* message_arena = GetArena();
  ::google::protobuf::internal::TSanWrite(&_impl_);
  if (message_arena == nullptr) {
    delete reinterpret_cast<::google::protobuf::MessageLite*>(_impl_.input_);
  }

  if (value != nullptr) {
    ::google::protobuf::Arena* submessage_arena = reinterpret_cast<::google::protobuf::MessageLite*>(value)->GetArena();
    if (message_arena != submessage_arena) {
      value = ::google::protobuf::internal::GetOwnedMessage(message_arena, value, submessage_arena);
    }
    _impl_._has_bits_[0] |= 0x00000001u;
  } else {
    _impl_._has_bits_[0] &= ~0x00000001u;
  }

  _impl_.input_ = reinterpret_cast<::tensorflow::serving::Input*>(value);
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.MultiInferenceRequest.input)
}

// -------------------------------------------------------------------

// MultiInferenceResponse

// repeated .tensorflow.serving.InferenceResult results = 1;
inline int MultiInferenceResponse::_internal_results_size() const {
  return _internal_results().size();
}
inline int MultiInferenceResponse::results_size() const {
  return _internal_results_size();
}
inline void MultiInferenceResponse::clear_results() {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  _impl_.results_.Clear();
}
inline ::tensorflow::serving::InferenceResult* MultiInferenceResponse::mutable_results(int index)
    ABSL_ATTRIBUTE_LIFETIME_BOUND {
  // @@protoc_insertion_point(field_mutable:tensorflow.serving.MultiInferenceResponse.results)
  return _internal_mutable_results()->Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField<::tensorflow::serving::InferenceResult>* MultiInferenceResponse::mutable_results()
    ABSL_ATTRIBUTE_LIFETIME_BOUND {
  // @@protoc_insertion_point(field_mutable_list:tensorflow.serving.MultiInferenceResponse.results)
  ::google::protobuf::internal::TSanWrite(&_impl_);
  return _internal_mutable_results();
}
inline const ::tensorflow::serving::InferenceResult& MultiInferenceResponse::results(int index) const
    ABSL_ATTRIBUTE_LIFETIME_BOUND {
  // @@protoc_insertion_point(field_get:tensorflow.serving.MultiInferenceResponse.results)
  return _internal_results().Get(index);
}
inline ::tensorflow::serving::InferenceResult* MultiInferenceResponse::add_results() ABSL_ATTRIBUTE_LIFETIME_BOUND {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  ::tensorflow::serving::InferenceResult* _add = _internal_mutable_results()->Add();
  // @@protoc_insertion_point(field_add:tensorflow.serving.MultiInferenceResponse.results)
  return _add;
}
inline const ::google::protobuf::RepeatedPtrField<::tensorflow::serving::InferenceResult>& MultiInferenceResponse::results() const
    ABSL_ATTRIBUTE_LIFETIME_BOUND {
  // @@protoc_insertion_point(field_list:tensorflow.serving.MultiInferenceResponse.results)
  return _internal_results();
}
inline const ::google::protobuf::RepeatedPtrField<::tensorflow::serving::InferenceResult>&
MultiInferenceResponse::_internal_results() const {
  ::google::protobuf::internal::TSanRead(&_impl_);
  return _impl_.results_;
}
inline ::google::protobuf::RepeatedPtrField<::tensorflow::serving::InferenceResult>*
MultiInferenceResponse::_internal_mutable_results() {
  ::google::protobuf::internal::TSanRead(&_impl_);
  return &_impl_.results_;
}

#ifdef __GNUC__
#pragma GCC diagnostic pop
#endif  // __GNUC__

// @@protoc_insertion_point(namespace_scope)
}  // namespace serving
}  // namespace tensorflow


// @@protoc_insertion_point(global_scope)

#include "google/protobuf/port_undef.inc"

#endif  // tensorflow_5fserving_2fapis_2finference_2eproto_2epb_2eh
