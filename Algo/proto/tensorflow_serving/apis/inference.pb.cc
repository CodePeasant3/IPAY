// Generated by the protocol buffer compiler.  DO NOT EDIT!
// NO CHECKED-IN PROTOBUF GENCODE
// source: tensorflow_serving/apis/inference.proto
// Protobuf C++ Version: 5.29.0

#include "tensorflow_serving/apis/inference.pb.h"

#include <algorithm>
#include <type_traits>
#include "google/protobuf/io/coded_stream.h"
#include "google/protobuf/generated_message_tctable_impl.h"
#include "google/protobuf/extension_set.h"
#include "google/protobuf/generated_message_util.h"
#include "google/protobuf/wire_format_lite.h"
#include "google/protobuf/descriptor.h"
#include "google/protobuf/generated_message_reflection.h"
#include "google/protobuf/reflection_ops.h"
#include "google/protobuf/wire_format.h"
// @@protoc_insertion_point(includes)

// Must be included last.
#include "google/protobuf/port_def.inc"
PROTOBUF_PRAGMA_INIT_SEG
namespace _pb = ::google::protobuf;
namespace _pbi = ::google::protobuf::internal;
namespace _fl = ::google::protobuf::internal::field_layout;
namespace tensorflow {
namespace serving {

inline constexpr InferenceTask::Impl_::Impl_(
    ::_pbi::ConstantInitialized) noexcept
      : _cached_size_{0},
        method_name_(
            &::google::protobuf::internal::fixed_address_empty_string,
            ::_pbi::ConstantInitialized()),
        model_spec_{nullptr} {}

template <typename>
PROTOBUF_CONSTEXPR InferenceTask::InferenceTask(::_pbi::ConstantInitialized)
#if defined(PROTOBUF_CUSTOM_VTABLE)
    : ::google::protobuf::Message(_class_data_.base()),
#else   // PROTOBUF_CUSTOM_VTABLE
    : ::google::protobuf::Message(),
#endif  // PROTOBUF_CUSTOM_VTABLE
      _impl_(::_pbi::ConstantInitialized()) {
}
struct InferenceTaskDefaultTypeInternal {
  PROTOBUF_CONSTEXPR InferenceTaskDefaultTypeInternal() : _instance(::_pbi::ConstantInitialized{}) {}
  ~InferenceTaskDefaultTypeInternal() {}
  union {
    InferenceTask _instance;
  };
};

PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT
    PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 InferenceTaskDefaultTypeInternal _InferenceTask_default_instance_;

inline constexpr InferenceResult::Impl_::Impl_(
    ::_pbi::ConstantInitialized) noexcept
      : _cached_size_{0},
        model_spec_{nullptr},
        result_{},
        _oneof_case_{} {}

template <typename>
PROTOBUF_CONSTEXPR InferenceResult::InferenceResult(::_pbi::ConstantInitialized)
#if defined(PROTOBUF_CUSTOM_VTABLE)
    : ::google::protobuf::Message(_class_data_.base()),
#else   // PROTOBUF_CUSTOM_VTABLE
    : ::google::protobuf::Message(),
#endif  // PROTOBUF_CUSTOM_VTABLE
      _impl_(::_pbi::ConstantInitialized()) {
}
struct InferenceResultDefaultTypeInternal {
  PROTOBUF_CONSTEXPR InferenceResultDefaultTypeInternal() : _instance(::_pbi::ConstantInitialized{}) {}
  ~InferenceResultDefaultTypeInternal() {}
  union {
    InferenceResult _instance;
  };
};

PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT
    PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 InferenceResultDefaultTypeInternal _InferenceResult_default_instance_;

inline constexpr MultiInferenceResponse::Impl_::Impl_(
    ::_pbi::ConstantInitialized) noexcept
      : results_{},
        _cached_size_{0} {}

template <typename>
PROTOBUF_CONSTEXPR MultiInferenceResponse::MultiInferenceResponse(::_pbi::ConstantInitialized)
#if defined(PROTOBUF_CUSTOM_VTABLE)
    : ::google::protobuf::Message(_class_data_.base()),
#else   // PROTOBUF_CUSTOM_VTABLE
    : ::google::protobuf::Message(),
#endif  // PROTOBUF_CUSTOM_VTABLE
      _impl_(::_pbi::ConstantInitialized()) {
}
struct MultiInferenceResponseDefaultTypeInternal {
  PROTOBUF_CONSTEXPR MultiInferenceResponseDefaultTypeInternal() : _instance(::_pbi::ConstantInitialized{}) {}
  ~MultiInferenceResponseDefaultTypeInternal() {}
  union {
    MultiInferenceResponse _instance;
  };
};

PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT
    PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 MultiInferenceResponseDefaultTypeInternal _MultiInferenceResponse_default_instance_;

inline constexpr MultiInferenceRequest::Impl_::Impl_(
    ::_pbi::ConstantInitialized) noexcept
      : _cached_size_{0},
        tasks_{},
        input_{nullptr} {}

template <typename>
PROTOBUF_CONSTEXPR MultiInferenceRequest::MultiInferenceRequest(::_pbi::ConstantInitialized)
#if defined(PROTOBUF_CUSTOM_VTABLE)
    : ::google::protobuf::Message(_class_data_.base()),
#else   // PROTOBUF_CUSTOM_VTABLE
    : ::google::protobuf::Message(),
#endif  // PROTOBUF_CUSTOM_VTABLE
      _impl_(::_pbi::ConstantInitialized()) {
}
struct MultiInferenceRequestDefaultTypeInternal {
  PROTOBUF_CONSTEXPR MultiInferenceRequestDefaultTypeInternal() : _instance(::_pbi::ConstantInitialized{}) {}
  ~MultiInferenceRequestDefaultTypeInternal() {}
  union {
    MultiInferenceRequest _instance;
  };
};

PROTOBUF_ATTRIBUTE_NO_DESTROY PROTOBUF_CONSTINIT
    PROTOBUF_ATTRIBUTE_INIT_PRIORITY1 MultiInferenceRequestDefaultTypeInternal _MultiInferenceRequest_default_instance_;
}  // namespace serving
}  // namespace tensorflow
static constexpr const ::_pb::EnumDescriptor**
    file_level_enum_descriptors_tensorflow_5fserving_2fapis_2finference_2eproto = nullptr;
static constexpr const ::_pb::ServiceDescriptor**
    file_level_service_descriptors_tensorflow_5fserving_2fapis_2finference_2eproto = nullptr;
const ::uint32_t
    TableStruct_tensorflow_5fserving_2fapis_2finference_2eproto::offsets[] ABSL_ATTRIBUTE_SECTION_VARIABLE(
        protodesc_cold) = {
        PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceTask, _impl_._has_bits_),
        PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceTask, _internal_metadata_),
        ~0u,  // no _extensions_
        ~0u,  // no _oneof_case_
        ~0u,  // no _weak_field_map_
        ~0u,  // no _inlined_string_donated_
        ~0u,  // no _split_
        ~0u,  // no sizeof(Split)
        PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceTask, _impl_.model_spec_),
        PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceTask, _impl_.method_name_),
        0,
        ~0u,
        PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceResult, _impl_._has_bits_),
        PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceResult, _internal_metadata_),
        ~0u,  // no _extensions_
        PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceResult, _impl_._oneof_case_[0]),
        ~0u,  // no _weak_field_map_
        ~0u,  // no _inlined_string_donated_
        ~0u,  // no _split_
        ~0u,  // no sizeof(Split)
        PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceResult, _impl_.model_spec_),
        ::_pbi::kInvalidFieldOffsetTag,
        ::_pbi::kInvalidFieldOffsetTag,
        PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceResult, _impl_.result_),
        0,
        ~0u,
        ~0u,
        PROTOBUF_FIELD_OFFSET(::tensorflow::serving::MultiInferenceRequest, _impl_._has_bits_),
        PROTOBUF_FIELD_OFFSET(::tensorflow::serving::MultiInferenceRequest, _internal_metadata_),
        ~0u,  // no _extensions_
        ~0u,  // no _oneof_case_
        ~0u,  // no _weak_field_map_
        ~0u,  // no _inlined_string_donated_
        ~0u,  // no _split_
        ~0u,  // no sizeof(Split)
        PROTOBUF_FIELD_OFFSET(::tensorflow::serving::MultiInferenceRequest, _impl_.tasks_),
        PROTOBUF_FIELD_OFFSET(::tensorflow::serving::MultiInferenceRequest, _impl_.input_),
        ~0u,
        0,
        ~0u,  // no _has_bits_
        PROTOBUF_FIELD_OFFSET(::tensorflow::serving::MultiInferenceResponse, _internal_metadata_),
        ~0u,  // no _extensions_
        ~0u,  // no _oneof_case_
        ~0u,  // no _weak_field_map_
        ~0u,  // no _inlined_string_donated_
        ~0u,  // no _split_
        ~0u,  // no sizeof(Split)
        PROTOBUF_FIELD_OFFSET(::tensorflow::serving::MultiInferenceResponse, _impl_.results_),
};

static const ::_pbi::MigrationSchema
    schemas[] ABSL_ATTRIBUTE_SECTION_VARIABLE(protodesc_cold) = {
        {0, 10, -1, sizeof(::tensorflow::serving::InferenceTask)},
        {12, 24, -1, sizeof(::tensorflow::serving::InferenceResult)},
        {27, 37, -1, sizeof(::tensorflow::serving::MultiInferenceRequest)},
        {39, -1, -1, sizeof(::tensorflow::serving::MultiInferenceResponse)},
};
static const ::_pb::Message* const file_default_instances[] = {
    &::tensorflow::serving::_InferenceTask_default_instance_._instance,
    &::tensorflow::serving::_InferenceResult_default_instance_._instance,
    &::tensorflow::serving::_MultiInferenceRequest_default_instance_._instance,
    &::tensorflow::serving::_MultiInferenceResponse_default_instance_._instance,
};
const char descriptor_table_protodef_tensorflow_5fserving_2fapis_2finference_2eproto[] ABSL_ATTRIBUTE_SECTION_VARIABLE(
    protodesc_cold) = {
    "\n\'tensorflow_serving/apis/inference.prot"
    "o\022\022tensorflow.serving\032,tensorflow_servin"
    "g/apis/classification.proto\032#tensorflow_"
    "serving/apis/input.proto\032#tensorflow_ser"
    "ving/apis/model.proto\032(tensorflow_servin"
    "g/apis/regression.proto\"W\n\rInferenceTask"
    "\0221\n\nmodel_spec\030\001 \001(\0132\035.tensorflow.servin"
    "g.ModelSpec\022\023\n\013method_name\030\002 \001(\t\"\334\001\n\017Inf"
    "erenceResult\0221\n\nmodel_spec\030\001 \001(\0132\035.tenso"
    "rflow.serving.ModelSpec\022I\n\025classificatio"
    "n_result\030\002 \001(\0132(.tensorflow.serving.Clas"
    "sificationResultH\000\022A\n\021regression_result\030"
    "\003 \001(\0132$.tensorflow.serving.RegressionRes"
    "ultH\000B\010\n\006result\"s\n\025MultiInferenceRequest"
    "\0220\n\005tasks\030\001 \003(\0132!.tensorflow.serving.Inf"
    "erenceTask\022(\n\005input\030\002 \001(\0132\031.tensorflow.s"
    "erving.Input\"N\n\026MultiInferenceResponse\0224"
    "\n\007results\030\001 \003(\0132#.tensorflow.serving.Inf"
    "erenceResultB\003\370\001\001b\006proto3"
};
static const ::_pbi::DescriptorTable* const descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto_deps[4] =
    {
        &::descriptor_table_tensorflow_5fserving_2fapis_2fclassification_2eproto,
        &::descriptor_table_tensorflow_5fserving_2fapis_2finput_2eproto,
        &::descriptor_table_tensorflow_5fserving_2fapis_2fmodel_2eproto,
        &::descriptor_table_tensorflow_5fserving_2fapis_2fregression_2eproto,
};
static ::absl::once_flag descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto_once;
PROTOBUF_CONSTINIT const ::_pbi::DescriptorTable descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto = {
    false,
    false,
    745,
    descriptor_table_protodef_tensorflow_5fserving_2fapis_2finference_2eproto,
    "tensorflow_serving/apis/inference.proto",
    &descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto_once,
    descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto_deps,
    4,
    4,
    schemas,
    file_default_instances,
    TableStruct_tensorflow_5fserving_2fapis_2finference_2eproto::offsets,
    file_level_enum_descriptors_tensorflow_5fserving_2fapis_2finference_2eproto,
    file_level_service_descriptors_tensorflow_5fserving_2fapis_2finference_2eproto,
};
namespace tensorflow {
namespace serving {
// ===================================================================

class InferenceTask::_Internal {
 public:
  using HasBits =
      decltype(std::declval<InferenceTask>()._impl_._has_bits_);
  static constexpr ::int32_t kHasBitsOffset =
      8 * PROTOBUF_FIELD_OFFSET(InferenceTask, _impl_._has_bits_);
};

void InferenceTask::clear_model_spec() {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  if (_impl_.model_spec_ != nullptr) _impl_.model_spec_->Clear();
  _impl_._has_bits_[0] &= ~0x00000001u;
}
InferenceTask::InferenceTask(::google::protobuf::Arena* arena)
#if defined(PROTOBUF_CUSTOM_VTABLE)
    : ::google::protobuf::Message(arena, _class_data_.base()) {
#else   // PROTOBUF_CUSTOM_VTABLE
    : ::google::protobuf::Message(arena) {
#endif  // PROTOBUF_CUSTOM_VTABLE
  SharedCtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.InferenceTask)
}
inline PROTOBUF_NDEBUG_INLINE InferenceTask::Impl_::Impl_(
    ::google::protobuf::internal::InternalVisibility visibility, ::google::protobuf::Arena* arena,
    const Impl_& from, const ::tensorflow::serving::InferenceTask& from_msg)
      : _has_bits_{from._has_bits_},
        _cached_size_{0},
        method_name_(arena, from.method_name_) {}

InferenceTask::InferenceTask(
    ::google::protobuf::Arena* arena,
    const InferenceTask& from)
#if defined(PROTOBUF_CUSTOM_VTABLE)
    : ::google::protobuf::Message(arena, _class_data_.base()) {
#else   // PROTOBUF_CUSTOM_VTABLE
    : ::google::protobuf::Message(arena) {
#endif  // PROTOBUF_CUSTOM_VTABLE
  InferenceTask* const _this = this;
  (void)_this;
  _internal_metadata_.MergeFrom<::google::protobuf::UnknownFieldSet>(
      from._internal_metadata_);
  new (&_impl_) Impl_(internal_visibility(), arena, from._impl_, from);
  ::uint32_t cached_has_bits = _impl_._has_bits_[0];
  _impl_.model_spec_ = (cached_has_bits & 0x00000001u) ? ::google::protobuf::Message::CopyConstruct<::tensorflow::serving::ModelSpec>(
                              arena, *from._impl_.model_spec_)
                        : nullptr;

  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.InferenceTask)
}
inline PROTOBUF_NDEBUG_INLINE InferenceTask::Impl_::Impl_(
    ::google::protobuf::internal::InternalVisibility visibility,
    ::google::protobuf::Arena* arena)
      : _cached_size_{0},
        method_name_(arena) {}

inline void InferenceTask::SharedCtor(::_pb::Arena* arena) {
  new (&_impl_) Impl_(internal_visibility(), arena);
  _impl_.model_spec_ = {};
}
InferenceTask::~InferenceTask() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.InferenceTask)
  SharedDtor(*this);
}
inline void InferenceTask::SharedDtor(MessageLite& self) {
  InferenceTask& this_ = static_cast<InferenceTask&>(self);
  this_._internal_metadata_.Delete<::google::protobuf::UnknownFieldSet>();
  ABSL_DCHECK(this_.GetArena() == nullptr);
  this_._impl_.method_name_.Destroy();
  delete this_._impl_.model_spec_;
  this_._impl_.~Impl_();
}

inline void* InferenceTask::PlacementNew_(const void*, void* mem,
                                        ::google::protobuf::Arena* arena) {
  return ::new (mem) InferenceTask(arena);
}
constexpr auto InferenceTask::InternalNewImpl_() {
  return ::google::protobuf::internal::MessageCreator::CopyInit(sizeof(InferenceTask),
                                            alignof(InferenceTask));
}
PROTOBUF_CONSTINIT
PROTOBUF_ATTRIBUTE_INIT_PRIORITY1
const ::google::protobuf::internal::ClassDataFull InferenceTask::_class_data_ = {
    ::google::protobuf::internal::ClassData{
        &_InferenceTask_default_instance_._instance,
        &_table_.header,
        nullptr,  // OnDemandRegisterArenaDtor
        nullptr,  // IsInitialized
        &InferenceTask::MergeImpl,
        ::google::protobuf::Message::GetNewImpl<InferenceTask>(),
#if defined(PROTOBUF_CUSTOM_VTABLE)
        &InferenceTask::SharedDtor,
        ::google::protobuf::Message::GetClearImpl<InferenceTask>(), &InferenceTask::ByteSizeLong,
            &InferenceTask::_InternalSerialize,
#endif  // PROTOBUF_CUSTOM_VTABLE
        PROTOBUF_FIELD_OFFSET(InferenceTask, _impl_._cached_size_),
        false,
    },
    &InferenceTask::kDescriptorMethods,
    &descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto,
    nullptr,  // tracker
};
const ::google::protobuf::internal::ClassData* InferenceTask::GetClassData() const {
  ::google::protobuf::internal::PrefetchToLocalCache(&_class_data_);
  ::google::protobuf::internal::PrefetchToLocalCache(_class_data_.tc_table);
  return _class_data_.base();
}
PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1
const ::_pbi::TcParseTable<1, 2, 1, 52, 2> InferenceTask::_table_ = {
  {
    PROTOBUF_FIELD_OFFSET(InferenceTask, _impl_._has_bits_),
    0, // no _extensions_
    2, 8,  // max_field_number, fast_idx_mask
    offsetof(decltype(_table_), field_lookup_table),
    4294967292,  // skipmap
    offsetof(decltype(_table_), field_entries),
    2,  // num_field_entries
    1,  // num_aux_entries
    offsetof(decltype(_table_), aux_entries),
    _class_data_.base(),
    nullptr,  // post_loop_handler
    ::_pbi::TcParser::GenericFallback,  // fallback
    #ifdef PROTOBUF_PREFETCH_PARSE_TABLE
    ::_pbi::TcParser::GetTable<::tensorflow::serving::InferenceTask>(),  // to_prefetch
    #endif  // PROTOBUF_PREFETCH_PARSE_TABLE
  }, {{
    // string method_name = 2;
    {::_pbi::TcParser::FastUS1,
     {18, 63, 0, PROTOBUF_FIELD_OFFSET(InferenceTask, _impl_.method_name_)}},
    // .tensorflow.serving.ModelSpec model_spec = 1;
    {::_pbi::TcParser::FastMtS1,
     {10, 0, 0, PROTOBUF_FIELD_OFFSET(InferenceTask, _impl_.model_spec_)}},
  }}, {{
    65535, 65535
  }}, {{
    // .tensorflow.serving.ModelSpec model_spec = 1;
    {PROTOBUF_FIELD_OFFSET(InferenceTask, _impl_.model_spec_), _Internal::kHasBitsOffset + 0, 0,
    (0 | ::_fl::kFcOptional | ::_fl::kMessage | ::_fl::kTvTable)},
    // string method_name = 2;
    {PROTOBUF_FIELD_OFFSET(InferenceTask, _impl_.method_name_), -1, 0,
    (0 | ::_fl::kFcSingular | ::_fl::kUtf8String | ::_fl::kRepAString)},
  }}, {{
    {::_pbi::TcParser::GetTable<::tensorflow::serving::ModelSpec>()},
  }}, {{
    "\40\0\13\0\0\0\0\0"
    "tensorflow.serving.InferenceTask"
    "method_name"
  }},
};

PROTOBUF_NOINLINE void InferenceTask::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.InferenceTask)
  ::google::protobuf::internal::TSanWrite(&_impl_);
  ::uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.method_name_.ClearToEmpty();
  cached_has_bits = _impl_._has_bits_[0];
  if (cached_has_bits & 0x00000001u) {
    ABSL_DCHECK(_impl_.model_spec_ != nullptr);
    _impl_.model_spec_->Clear();
  }
  _impl_._has_bits_.Clear();
  _internal_metadata_.Clear<::google::protobuf::UnknownFieldSet>();
}

#if defined(PROTOBUF_CUSTOM_VTABLE)
        ::uint8_t* InferenceTask::_InternalSerialize(
            const MessageLite& base, ::uint8_t* target,
            ::google::protobuf::io::EpsCopyOutputStream* stream) {
          const InferenceTask& this_ = static_cast<const InferenceTask&>(base);
#else   // PROTOBUF_CUSTOM_VTABLE
        ::uint8_t* InferenceTask::_InternalSerialize(
            ::uint8_t* target,
            ::google::protobuf::io::EpsCopyOutputStream* stream) const {
          const InferenceTask& this_ = *this;
#endif  // PROTOBUF_CUSTOM_VTABLE
          // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.InferenceTask)
          ::uint32_t cached_has_bits = 0;
          (void)cached_has_bits;

          cached_has_bits = this_._impl_._has_bits_[0];
          // .tensorflow.serving.ModelSpec model_spec = 1;
          if (cached_has_bits & 0x00000001u) {
            target = ::google::protobuf::internal::WireFormatLite::InternalWriteMessage(
                1, *this_._impl_.model_spec_, this_._impl_.model_spec_->GetCachedSize(), target,
                stream);
          }

          // string method_name = 2;
          if (!this_._internal_method_name().empty()) {
            const std::string& _s = this_._internal_method_name();
            ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
                _s.data(), static_cast<int>(_s.length()), ::google::protobuf::internal::WireFormatLite::SERIALIZE, "tensorflow.serving.InferenceTask.method_name");
            target = stream->WriteStringMaybeAliased(2, _s, target);
          }

          if (PROTOBUF_PREDICT_FALSE(this_._internal_metadata_.have_unknown_fields())) {
            target =
                ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
                    this_._internal_metadata_.unknown_fields<::google::protobuf::UnknownFieldSet>(::google::protobuf::UnknownFieldSet::default_instance), target, stream);
          }
          // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.InferenceTask)
          return target;
        }

#if defined(PROTOBUF_CUSTOM_VTABLE)
        ::size_t InferenceTask::ByteSizeLong(const MessageLite& base) {
          const InferenceTask& this_ = static_cast<const InferenceTask&>(base);
#else   // PROTOBUF_CUSTOM_VTABLE
        ::size_t InferenceTask::ByteSizeLong() const {
          const InferenceTask& this_ = *this;
#endif  // PROTOBUF_CUSTOM_VTABLE
          // @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.InferenceTask)
          ::size_t total_size = 0;

          ::uint32_t cached_has_bits = 0;
          // Prevent compiler warnings about cached_has_bits being unused
          (void)cached_has_bits;

          ::_pbi::Prefetch5LinesFrom7Lines(&this_);
           {
            // string method_name = 2;
            if (!this_._internal_method_name().empty()) {
              total_size += 1 + ::google::protobuf::internal::WireFormatLite::StringSize(
                                              this_._internal_method_name());
            }
          }
           {
            // .tensorflow.serving.ModelSpec model_spec = 1;
            cached_has_bits = this_._impl_._has_bits_[0];
            if (cached_has_bits & 0x00000001u) {
              total_size += 1 +
                            ::google::protobuf::internal::WireFormatLite::MessageSize(*this_._impl_.model_spec_);
            }
          }
          return this_.MaybeComputeUnknownFieldsSize(total_size,
                                                     &this_._impl_._cached_size_);
        }

void InferenceTask::MergeImpl(::google::protobuf::MessageLite& to_msg, const ::google::protobuf::MessageLite& from_msg) {
  auto* const _this = static_cast<InferenceTask*>(&to_msg);
  auto& from = static_cast<const InferenceTask&>(from_msg);
  ::google::protobuf::Arena* arena = _this->GetArena();
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.InferenceTask)
  ABSL_DCHECK_NE(&from, _this);
  ::uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  if (!from._internal_method_name().empty()) {
    _this->_internal_set_method_name(from._internal_method_name());
  }
  cached_has_bits = from._impl_._has_bits_[0];
  if (cached_has_bits & 0x00000001u) {
    ABSL_DCHECK(from._impl_.model_spec_ != nullptr);
    if (_this->_impl_.model_spec_ == nullptr) {
      _this->_impl_.model_spec_ =
          ::google::protobuf::Message::CopyConstruct<::tensorflow::serving::ModelSpec>(arena, *from._impl_.model_spec_);
    } else {
      _this->_impl_.model_spec_->MergeFrom(*from._impl_.model_spec_);
    }
  }
  _this->_impl_._has_bits_[0] |= cached_has_bits;
  _this->_internal_metadata_.MergeFrom<::google::protobuf::UnknownFieldSet>(from._internal_metadata_);
}

void InferenceTask::CopyFrom(const InferenceTask& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.InferenceTask)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}


void InferenceTask::InternalSwap(InferenceTask* PROTOBUF_RESTRICT other) {
  using std::swap;
  auto* arena = GetArena();
  ABSL_DCHECK_EQ(arena, other->GetArena());
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  swap(_impl_._has_bits_[0], other->_impl_._has_bits_[0]);
  ::_pbi::ArenaStringPtr::InternalSwap(&_impl_.method_name_, &other->_impl_.method_name_, arena);
  swap(_impl_.model_spec_, other->_impl_.model_spec_);
}

::google::protobuf::Metadata InferenceTask::GetMetadata() const {
  return ::google::protobuf::Message::GetMetadataImpl(GetClassData()->full());
}
// ===================================================================

class InferenceResult::_Internal {
 public:
  using HasBits =
      decltype(std::declval<InferenceResult>()._impl_._has_bits_);
  static constexpr ::int32_t kHasBitsOffset =
      8 * PROTOBUF_FIELD_OFFSET(InferenceResult, _impl_._has_bits_);
  static constexpr ::int32_t kOneofCaseOffset =
      PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceResult, _impl_._oneof_case_);
};

void InferenceResult::clear_model_spec() {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  if (_impl_.model_spec_ != nullptr) _impl_.model_spec_->Clear();
  _impl_._has_bits_[0] &= ~0x00000001u;
}
void InferenceResult::set_allocated_classification_result(::tensorflow::serving::ClassificationResult* classification_result) {
  ::google::protobuf::Arena* message_arena = GetArena();
  clear_result();
  if (classification_result) {
    ::google::protobuf::Arena* submessage_arena = reinterpret_cast<::google::protobuf::MessageLite*>(classification_result)->GetArena();
    if (message_arena != submessage_arena) {
      classification_result = ::google::protobuf::internal::GetOwnedMessage(message_arena, classification_result, submessage_arena);
    }
    set_has_classification_result();
    _impl_.result_.classification_result_ = classification_result;
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.InferenceResult.classification_result)
}
void InferenceResult::clear_classification_result() {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  if (result_case() == kClassificationResult) {
    if (GetArena() == nullptr) {
      delete _impl_.result_.classification_result_;
    } else if (::google::protobuf::internal::DebugHardenClearOneofMessageOnArena()) {
      ::google::protobuf::internal::MaybePoisonAfterClear(_impl_.result_.classification_result_);
    }
    clear_has_result();
  }
}
void InferenceResult::set_allocated_regression_result(::tensorflow::serving::RegressionResult* regression_result) {
  ::google::protobuf::Arena* message_arena = GetArena();
  clear_result();
  if (regression_result) {
    ::google::protobuf::Arena* submessage_arena = reinterpret_cast<::google::protobuf::MessageLite*>(regression_result)->GetArena();
    if (message_arena != submessage_arena) {
      regression_result = ::google::protobuf::internal::GetOwnedMessage(message_arena, regression_result, submessage_arena);
    }
    set_has_regression_result();
    _impl_.result_.regression_result_ = regression_result;
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.InferenceResult.regression_result)
}
void InferenceResult::clear_regression_result() {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  if (result_case() == kRegressionResult) {
    if (GetArena() == nullptr) {
      delete _impl_.result_.regression_result_;
    } else if (::google::protobuf::internal::DebugHardenClearOneofMessageOnArena()) {
      ::google::protobuf::internal::MaybePoisonAfterClear(_impl_.result_.regression_result_);
    }
    clear_has_result();
  }
}
InferenceResult::InferenceResult(::google::protobuf::Arena* arena)
#if defined(PROTOBUF_CUSTOM_VTABLE)
    : ::google::protobuf::Message(arena, _class_data_.base()) {
#else   // PROTOBUF_CUSTOM_VTABLE
    : ::google::protobuf::Message(arena) {
#endif  // PROTOBUF_CUSTOM_VTABLE
  SharedCtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.InferenceResult)
}
inline PROTOBUF_NDEBUG_INLINE InferenceResult::Impl_::Impl_(
    ::google::protobuf::internal::InternalVisibility visibility, ::google::protobuf::Arena* arena,
    const Impl_& from, const ::tensorflow::serving::InferenceResult& from_msg)
      : _has_bits_{from._has_bits_},
        _cached_size_{0},
        result_{},
        _oneof_case_{from._oneof_case_[0]} {}

InferenceResult::InferenceResult(
    ::google::protobuf::Arena* arena,
    const InferenceResult& from)
#if defined(PROTOBUF_CUSTOM_VTABLE)
    : ::google::protobuf::Message(arena, _class_data_.base()) {
#else   // PROTOBUF_CUSTOM_VTABLE
    : ::google::protobuf::Message(arena) {
#endif  // PROTOBUF_CUSTOM_VTABLE
  InferenceResult* const _this = this;
  (void)_this;
  _internal_metadata_.MergeFrom<::google::protobuf::UnknownFieldSet>(
      from._internal_metadata_);
  new (&_impl_) Impl_(internal_visibility(), arena, from._impl_, from);
  ::uint32_t cached_has_bits = _impl_._has_bits_[0];
  _impl_.model_spec_ = (cached_has_bits & 0x00000001u) ? ::google::protobuf::Message::CopyConstruct<::tensorflow::serving::ModelSpec>(
                              arena, *from._impl_.model_spec_)
                        : nullptr;
  switch (result_case()) {
    case RESULT_NOT_SET:
      break;
      case kClassificationResult:
        _impl_.result_.classification_result_ = ::google::protobuf::Message::CopyConstruct<::tensorflow::serving::ClassificationResult>(arena, *from._impl_.result_.classification_result_);
        break;
      case kRegressionResult:
        _impl_.result_.regression_result_ = ::google::protobuf::Message::CopyConstruct<::tensorflow::serving::RegressionResult>(arena, *from._impl_.result_.regression_result_);
        break;
  }

  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.InferenceResult)
}
inline PROTOBUF_NDEBUG_INLINE InferenceResult::Impl_::Impl_(
    ::google::protobuf::internal::InternalVisibility visibility,
    ::google::protobuf::Arena* arena)
      : _cached_size_{0},
        result_{},
        _oneof_case_{} {}

inline void InferenceResult::SharedCtor(::_pb::Arena* arena) {
  new (&_impl_) Impl_(internal_visibility(), arena);
  _impl_.model_spec_ = {};
}
InferenceResult::~InferenceResult() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.InferenceResult)
  SharedDtor(*this);
}
inline void InferenceResult::SharedDtor(MessageLite& self) {
  InferenceResult& this_ = static_cast<InferenceResult&>(self);
  this_._internal_metadata_.Delete<::google::protobuf::UnknownFieldSet>();
  ABSL_DCHECK(this_.GetArena() == nullptr);
  delete this_._impl_.model_spec_;
  if (this_.has_result()) {
    this_.clear_result();
  }
  this_._impl_.~Impl_();
}

void InferenceResult::clear_result() {
// @@protoc_insertion_point(one_of_clear_start:tensorflow.serving.InferenceResult)
  ::google::protobuf::internal::TSanWrite(&_impl_);
  switch (result_case()) {
    case kClassificationResult: {
      if (GetArena() == nullptr) {
        delete _impl_.result_.classification_result_;
      } else if (::google::protobuf::internal::DebugHardenClearOneofMessageOnArena()) {
        ::google::protobuf::internal::MaybePoisonAfterClear(_impl_.result_.classification_result_);
      }
      break;
    }
    case kRegressionResult: {
      if (GetArena() == nullptr) {
        delete _impl_.result_.regression_result_;
      } else if (::google::protobuf::internal::DebugHardenClearOneofMessageOnArena()) {
        ::google::protobuf::internal::MaybePoisonAfterClear(_impl_.result_.regression_result_);
      }
      break;
    }
    case RESULT_NOT_SET: {
      break;
    }
  }
  _impl_._oneof_case_[0] = RESULT_NOT_SET;
}


inline void* InferenceResult::PlacementNew_(const void*, void* mem,
                                        ::google::protobuf::Arena* arena) {
  return ::new (mem) InferenceResult(arena);
}
constexpr auto InferenceResult::InternalNewImpl_() {
  return ::google::protobuf::internal::MessageCreator::ZeroInit(sizeof(InferenceResult),
                                            alignof(InferenceResult));
}
PROTOBUF_CONSTINIT
PROTOBUF_ATTRIBUTE_INIT_PRIORITY1
const ::google::protobuf::internal::ClassDataFull InferenceResult::_class_data_ = {
    ::google::protobuf::internal::ClassData{
        &_InferenceResult_default_instance_._instance,
        &_table_.header,
        nullptr,  // OnDemandRegisterArenaDtor
        nullptr,  // IsInitialized
        &InferenceResult::MergeImpl,
        ::google::protobuf::Message::GetNewImpl<InferenceResult>(),
#if defined(PROTOBUF_CUSTOM_VTABLE)
        &InferenceResult::SharedDtor,
        ::google::protobuf::Message::GetClearImpl<InferenceResult>(), &InferenceResult::ByteSizeLong,
            &InferenceResult::_InternalSerialize,
#endif  // PROTOBUF_CUSTOM_VTABLE
        PROTOBUF_FIELD_OFFSET(InferenceResult, _impl_._cached_size_),
        false,
    },
    &InferenceResult::kDescriptorMethods,
    &descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto,
    nullptr,  // tracker
};
const ::google::protobuf::internal::ClassData* InferenceResult::GetClassData() const {
  ::google::protobuf::internal::PrefetchToLocalCache(&_class_data_);
  ::google::protobuf::internal::PrefetchToLocalCache(_class_data_.tc_table);
  return _class_data_.base();
}
PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1
const ::_pbi::TcParseTable<0, 3, 3, 0, 2> InferenceResult::_table_ = {
  {
    PROTOBUF_FIELD_OFFSET(InferenceResult, _impl_._has_bits_),
    0, // no _extensions_
    3, 0,  // max_field_number, fast_idx_mask
    offsetof(decltype(_table_), field_lookup_table),
    4294967288,  // skipmap
    offsetof(decltype(_table_), field_entries),
    3,  // num_field_entries
    3,  // num_aux_entries
    offsetof(decltype(_table_), aux_entries),
    _class_data_.base(),
    nullptr,  // post_loop_handler
    ::_pbi::TcParser::GenericFallback,  // fallback
    #ifdef PROTOBUF_PREFETCH_PARSE_TABLE
    ::_pbi::TcParser::GetTable<::tensorflow::serving::InferenceResult>(),  // to_prefetch
    #endif  // PROTOBUF_PREFETCH_PARSE_TABLE
  }, {{
    // .tensorflow.serving.ModelSpec model_spec = 1;
    {::_pbi::TcParser::FastMtS1,
     {10, 0, 0, PROTOBUF_FIELD_OFFSET(InferenceResult, _impl_.model_spec_)}},
  }}, {{
    65535, 65535
  }}, {{
    // .tensorflow.serving.ModelSpec model_spec = 1;
    {PROTOBUF_FIELD_OFFSET(InferenceResult, _impl_.model_spec_), _Internal::kHasBitsOffset + 0, 0,
    (0 | ::_fl::kFcOptional | ::_fl::kMessage | ::_fl::kTvTable)},
    // .tensorflow.serving.ClassificationResult classification_result = 2;
    {PROTOBUF_FIELD_OFFSET(InferenceResult, _impl_.result_.classification_result_), _Internal::kOneofCaseOffset + 0, 1,
    (0 | ::_fl::kFcOneof | ::_fl::kMessage | ::_fl::kTvTable)},
    // .tensorflow.serving.RegressionResult regression_result = 3;
    {PROTOBUF_FIELD_OFFSET(InferenceResult, _impl_.result_.regression_result_), _Internal::kOneofCaseOffset + 0, 2,
    (0 | ::_fl::kFcOneof | ::_fl::kMessage | ::_fl::kTvTable)},
  }}, {{
    {::_pbi::TcParser::GetTable<::tensorflow::serving::ModelSpec>()},
    {::_pbi::TcParser::GetTable<::tensorflow::serving::ClassificationResult>()},
    {::_pbi::TcParser::GetTable<::tensorflow::serving::RegressionResult>()},
  }}, {{
  }},
};

PROTOBUF_NOINLINE void InferenceResult::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.InferenceResult)
  ::google::protobuf::internal::TSanWrite(&_impl_);
  ::uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  cached_has_bits = _impl_._has_bits_[0];
  if (cached_has_bits & 0x00000001u) {
    ABSL_DCHECK(_impl_.model_spec_ != nullptr);
    _impl_.model_spec_->Clear();
  }
  clear_result();
  _impl_._has_bits_.Clear();
  _internal_metadata_.Clear<::google::protobuf::UnknownFieldSet>();
}

#if defined(PROTOBUF_CUSTOM_VTABLE)
        ::uint8_t* InferenceResult::_InternalSerialize(
            const MessageLite& base, ::uint8_t* target,
            ::google::protobuf::io::EpsCopyOutputStream* stream) {
          const InferenceResult& this_ = static_cast<const InferenceResult&>(base);
#else   // PROTOBUF_CUSTOM_VTABLE
        ::uint8_t* InferenceResult::_InternalSerialize(
            ::uint8_t* target,
            ::google::protobuf::io::EpsCopyOutputStream* stream) const {
          const InferenceResult& this_ = *this;
#endif  // PROTOBUF_CUSTOM_VTABLE
          // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.InferenceResult)
          ::uint32_t cached_has_bits = 0;
          (void)cached_has_bits;

          cached_has_bits = this_._impl_._has_bits_[0];
          // .tensorflow.serving.ModelSpec model_spec = 1;
          if (cached_has_bits & 0x00000001u) {
            target = ::google::protobuf::internal::WireFormatLite::InternalWriteMessage(
                1, *this_._impl_.model_spec_, this_._impl_.model_spec_->GetCachedSize(), target,
                stream);
          }

          switch (this_.result_case()) {
            case kClassificationResult: {
              target = ::google::protobuf::internal::WireFormatLite::InternalWriteMessage(
                  2, *this_._impl_.result_.classification_result_, this_._impl_.result_.classification_result_->GetCachedSize(), target,
                  stream);
              break;
            }
            case kRegressionResult: {
              target = ::google::protobuf::internal::WireFormatLite::InternalWriteMessage(
                  3, *this_._impl_.result_.regression_result_, this_._impl_.result_.regression_result_->GetCachedSize(), target,
                  stream);
              break;
            }
            default:
              break;
          }
          if (PROTOBUF_PREDICT_FALSE(this_._internal_metadata_.have_unknown_fields())) {
            target =
                ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
                    this_._internal_metadata_.unknown_fields<::google::protobuf::UnknownFieldSet>(::google::protobuf::UnknownFieldSet::default_instance), target, stream);
          }
          // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.InferenceResult)
          return target;
        }

#if defined(PROTOBUF_CUSTOM_VTABLE)
        ::size_t InferenceResult::ByteSizeLong(const MessageLite& base) {
          const InferenceResult& this_ = static_cast<const InferenceResult&>(base);
#else   // PROTOBUF_CUSTOM_VTABLE
        ::size_t InferenceResult::ByteSizeLong() const {
          const InferenceResult& this_ = *this;
#endif  // PROTOBUF_CUSTOM_VTABLE
          // @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.InferenceResult)
          ::size_t total_size = 0;

          ::uint32_t cached_has_bits = 0;
          // Prevent compiler warnings about cached_has_bits being unused
          (void)cached_has_bits;

           {
            // .tensorflow.serving.ModelSpec model_spec = 1;
            cached_has_bits = this_._impl_._has_bits_[0];
            if (cached_has_bits & 0x00000001u) {
              total_size += 1 +
                            ::google::protobuf::internal::WireFormatLite::MessageSize(*this_._impl_.model_spec_);
            }
          }
          switch (this_.result_case()) {
            // .tensorflow.serving.ClassificationResult classification_result = 2;
            case kClassificationResult: {
              total_size += 1 +
                            ::google::protobuf::internal::WireFormatLite::MessageSize(*this_._impl_.result_.classification_result_);
              break;
            }
            // .tensorflow.serving.RegressionResult regression_result = 3;
            case kRegressionResult: {
              total_size += 1 +
                            ::google::protobuf::internal::WireFormatLite::MessageSize(*this_._impl_.result_.regression_result_);
              break;
            }
            case RESULT_NOT_SET: {
              break;
            }
          }
          return this_.MaybeComputeUnknownFieldsSize(total_size,
                                                     &this_._impl_._cached_size_);
        }

void InferenceResult::MergeImpl(::google::protobuf::MessageLite& to_msg, const ::google::protobuf::MessageLite& from_msg) {
  auto* const _this = static_cast<InferenceResult*>(&to_msg);
  auto& from = static_cast<const InferenceResult&>(from_msg);
  ::google::protobuf::Arena* arena = _this->GetArena();
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.InferenceResult)
  ABSL_DCHECK_NE(&from, _this);
  ::uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = from._impl_._has_bits_[0];
  if (cached_has_bits & 0x00000001u) {
    ABSL_DCHECK(from._impl_.model_spec_ != nullptr);
    if (_this->_impl_.model_spec_ == nullptr) {
      _this->_impl_.model_spec_ =
          ::google::protobuf::Message::CopyConstruct<::tensorflow::serving::ModelSpec>(arena, *from._impl_.model_spec_);
    } else {
      _this->_impl_.model_spec_->MergeFrom(*from._impl_.model_spec_);
    }
  }
  _this->_impl_._has_bits_[0] |= cached_has_bits;
  if (const uint32_t oneof_from_case = from._impl_._oneof_case_[0]) {
    const uint32_t oneof_to_case = _this->_impl_._oneof_case_[0];
    const bool oneof_needs_init = oneof_to_case != oneof_from_case;
    if (oneof_needs_init) {
      if (oneof_to_case != 0) {
        _this->clear_result();
      }
      _this->_impl_._oneof_case_[0] = oneof_from_case;
    }

    switch (oneof_from_case) {
      case kClassificationResult: {
        if (oneof_needs_init) {
          _this->_impl_.result_.classification_result_ =
              ::google::protobuf::Message::CopyConstruct<::tensorflow::serving::ClassificationResult>(arena, *from._impl_.result_.classification_result_);
        } else {
          _this->_impl_.result_.classification_result_->MergeFrom(from._internal_classification_result());
        }
        break;
      }
      case kRegressionResult: {
        if (oneof_needs_init) {
          _this->_impl_.result_.regression_result_ =
              ::google::protobuf::Message::CopyConstruct<::tensorflow::serving::RegressionResult>(arena, *from._impl_.result_.regression_result_);
        } else {
          _this->_impl_.result_.regression_result_->MergeFrom(from._internal_regression_result());
        }
        break;
      }
      case RESULT_NOT_SET:
        break;
    }
  }
  _this->_internal_metadata_.MergeFrom<::google::protobuf::UnknownFieldSet>(from._internal_metadata_);
}

void InferenceResult::CopyFrom(const InferenceResult& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.InferenceResult)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}


void InferenceResult::InternalSwap(InferenceResult* PROTOBUF_RESTRICT other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  swap(_impl_._has_bits_[0], other->_impl_._has_bits_[0]);
  swap(_impl_.model_spec_, other->_impl_.model_spec_);
  swap(_impl_.result_, other->_impl_.result_);
  swap(_impl_._oneof_case_[0], other->_impl_._oneof_case_[0]);
}

::google::protobuf::Metadata InferenceResult::GetMetadata() const {
  return ::google::protobuf::Message::GetMetadataImpl(GetClassData()->full());
}
// ===================================================================

class MultiInferenceRequest::_Internal {
 public:
  using HasBits =
      decltype(std::declval<MultiInferenceRequest>()._impl_._has_bits_);
  static constexpr ::int32_t kHasBitsOffset =
      8 * PROTOBUF_FIELD_OFFSET(MultiInferenceRequest, _impl_._has_bits_);
};

void MultiInferenceRequest::clear_input() {
  ::google::protobuf::internal::TSanWrite(&_impl_);
  if (_impl_.input_ != nullptr) _impl_.input_->Clear();
  _impl_._has_bits_[0] &= ~0x00000001u;
}
MultiInferenceRequest::MultiInferenceRequest(::google::protobuf::Arena* arena)
#if defined(PROTOBUF_CUSTOM_VTABLE)
    : ::google::protobuf::Message(arena, _class_data_.base()) {
#else   // PROTOBUF_CUSTOM_VTABLE
    : ::google::protobuf::Message(arena) {
#endif  // PROTOBUF_CUSTOM_VTABLE
  SharedCtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.MultiInferenceRequest)
}
inline PROTOBUF_NDEBUG_INLINE MultiInferenceRequest::Impl_::Impl_(
    ::google::protobuf::internal::InternalVisibility visibility, ::google::protobuf::Arena* arena,
    const Impl_& from, const ::tensorflow::serving::MultiInferenceRequest& from_msg)
      : _has_bits_{from._has_bits_},
        _cached_size_{0},
        tasks_{visibility, arena, from.tasks_} {}

MultiInferenceRequest::MultiInferenceRequest(
    ::google::protobuf::Arena* arena,
    const MultiInferenceRequest& from)
#if defined(PROTOBUF_CUSTOM_VTABLE)
    : ::google::protobuf::Message(arena, _class_data_.base()) {
#else   // PROTOBUF_CUSTOM_VTABLE
    : ::google::protobuf::Message(arena) {
#endif  // PROTOBUF_CUSTOM_VTABLE
  MultiInferenceRequest* const _this = this;
  (void)_this;
  _internal_metadata_.MergeFrom<::google::protobuf::UnknownFieldSet>(
      from._internal_metadata_);
  new (&_impl_) Impl_(internal_visibility(), arena, from._impl_, from);
  ::uint32_t cached_has_bits = _impl_._has_bits_[0];
  _impl_.input_ = (cached_has_bits & 0x00000001u) ? ::google::protobuf::Message::CopyConstruct<::tensorflow::serving::Input>(
                              arena, *from._impl_.input_)
                        : nullptr;

  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.MultiInferenceRequest)
}
inline PROTOBUF_NDEBUG_INLINE MultiInferenceRequest::Impl_::Impl_(
    ::google::protobuf::internal::InternalVisibility visibility,
    ::google::protobuf::Arena* arena)
      : _cached_size_{0},
        tasks_{visibility, arena} {}

inline void MultiInferenceRequest::SharedCtor(::_pb::Arena* arena) {
  new (&_impl_) Impl_(internal_visibility(), arena);
  _impl_.input_ = {};
}
MultiInferenceRequest::~MultiInferenceRequest() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.MultiInferenceRequest)
  SharedDtor(*this);
}
inline void MultiInferenceRequest::SharedDtor(MessageLite& self) {
  MultiInferenceRequest& this_ = static_cast<MultiInferenceRequest&>(self);
  this_._internal_metadata_.Delete<::google::protobuf::UnknownFieldSet>();
  ABSL_DCHECK(this_.GetArena() == nullptr);
  delete this_._impl_.input_;
  this_._impl_.~Impl_();
}

inline void* MultiInferenceRequest::PlacementNew_(const void*, void* mem,
                                        ::google::protobuf::Arena* arena) {
  return ::new (mem) MultiInferenceRequest(arena);
}
constexpr auto MultiInferenceRequest::InternalNewImpl_() {
  constexpr auto arena_bits = ::google::protobuf::internal::EncodePlacementArenaOffsets({
      PROTOBUF_FIELD_OFFSET(MultiInferenceRequest, _impl_.tasks_) +
          decltype(MultiInferenceRequest::_impl_.tasks_)::
              InternalGetArenaOffset(
                  ::google::protobuf::Message::internal_visibility()),
  });
  if (arena_bits.has_value()) {
    return ::google::protobuf::internal::MessageCreator::ZeroInit(
        sizeof(MultiInferenceRequest), alignof(MultiInferenceRequest), *arena_bits);
  } else {
    return ::google::protobuf::internal::MessageCreator(&MultiInferenceRequest::PlacementNew_,
                                 sizeof(MultiInferenceRequest),
                                 alignof(MultiInferenceRequest));
  }
}
PROTOBUF_CONSTINIT
PROTOBUF_ATTRIBUTE_INIT_PRIORITY1
const ::google::protobuf::internal::ClassDataFull MultiInferenceRequest::_class_data_ = {
    ::google::protobuf::internal::ClassData{
        &_MultiInferenceRequest_default_instance_._instance,
        &_table_.header,
        nullptr,  // OnDemandRegisterArenaDtor
        nullptr,  // IsInitialized
        &MultiInferenceRequest::MergeImpl,
        ::google::protobuf::Message::GetNewImpl<MultiInferenceRequest>(),
#if defined(PROTOBUF_CUSTOM_VTABLE)
        &MultiInferenceRequest::SharedDtor,
        ::google::protobuf::Message::GetClearImpl<MultiInferenceRequest>(), &MultiInferenceRequest::ByteSizeLong,
            &MultiInferenceRequest::_InternalSerialize,
#endif  // PROTOBUF_CUSTOM_VTABLE
        PROTOBUF_FIELD_OFFSET(MultiInferenceRequest, _impl_._cached_size_),
        false,
    },
    &MultiInferenceRequest::kDescriptorMethods,
    &descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto,
    nullptr,  // tracker
};
const ::google::protobuf::internal::ClassData* MultiInferenceRequest::GetClassData() const {
  ::google::protobuf::internal::PrefetchToLocalCache(&_class_data_);
  ::google::protobuf::internal::PrefetchToLocalCache(_class_data_.tc_table);
  return _class_data_.base();
}
PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1
const ::_pbi::TcParseTable<1, 2, 2, 0, 2> MultiInferenceRequest::_table_ = {
  {
    PROTOBUF_FIELD_OFFSET(MultiInferenceRequest, _impl_._has_bits_),
    0, // no _extensions_
    2, 8,  // max_field_number, fast_idx_mask
    offsetof(decltype(_table_), field_lookup_table),
    4294967292,  // skipmap
    offsetof(decltype(_table_), field_entries),
    2,  // num_field_entries
    2,  // num_aux_entries
    offsetof(decltype(_table_), aux_entries),
    _class_data_.base(),
    nullptr,  // post_loop_handler
    ::_pbi::TcParser::GenericFallback,  // fallback
    #ifdef PROTOBUF_PREFETCH_PARSE_TABLE
    ::_pbi::TcParser::GetTable<::tensorflow::serving::MultiInferenceRequest>(),  // to_prefetch
    #endif  // PROTOBUF_PREFETCH_PARSE_TABLE
  }, {{
    // .tensorflow.serving.Input input = 2;
    {::_pbi::TcParser::FastMtS1,
     {18, 0, 1, PROTOBUF_FIELD_OFFSET(MultiInferenceRequest, _impl_.input_)}},
    // repeated .tensorflow.serving.InferenceTask tasks = 1;
    {::_pbi::TcParser::FastMtR1,
     {10, 63, 0, PROTOBUF_FIELD_OFFSET(MultiInferenceRequest, _impl_.tasks_)}},
  }}, {{
    65535, 65535
  }}, {{
    // repeated .tensorflow.serving.InferenceTask tasks = 1;
    {PROTOBUF_FIELD_OFFSET(MultiInferenceRequest, _impl_.tasks_), -1, 0,
    (0 | ::_fl::kFcRepeated | ::_fl::kMessage | ::_fl::kTvTable)},
    // .tensorflow.serving.Input input = 2;
    {PROTOBUF_FIELD_OFFSET(MultiInferenceRequest, _impl_.input_), _Internal::kHasBitsOffset + 0, 1,
    (0 | ::_fl::kFcOptional | ::_fl::kMessage | ::_fl::kTvTable)},
  }}, {{
    {::_pbi::TcParser::GetTable<::tensorflow::serving::InferenceTask>()},
    {::_pbi::TcParser::GetTable<::tensorflow::serving::Input>()},
  }}, {{
  }},
};

PROTOBUF_NOINLINE void MultiInferenceRequest::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.MultiInferenceRequest)
  ::google::protobuf::internal::TSanWrite(&_impl_);
  ::uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.tasks_.Clear();
  cached_has_bits = _impl_._has_bits_[0];
  if (cached_has_bits & 0x00000001u) {
    ABSL_DCHECK(_impl_.input_ != nullptr);
    _impl_.input_->Clear();
  }
  _impl_._has_bits_.Clear();
  _internal_metadata_.Clear<::google::protobuf::UnknownFieldSet>();
}

#if defined(PROTOBUF_CUSTOM_VTABLE)
        ::uint8_t* MultiInferenceRequest::_InternalSerialize(
            const MessageLite& base, ::uint8_t* target,
            ::google::protobuf::io::EpsCopyOutputStream* stream) {
          const MultiInferenceRequest& this_ = static_cast<const MultiInferenceRequest&>(base);
#else   // PROTOBUF_CUSTOM_VTABLE
        ::uint8_t* MultiInferenceRequest::_InternalSerialize(
            ::uint8_t* target,
            ::google::protobuf::io::EpsCopyOutputStream* stream) const {
          const MultiInferenceRequest& this_ = *this;
#endif  // PROTOBUF_CUSTOM_VTABLE
          // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.MultiInferenceRequest)
          ::uint32_t cached_has_bits = 0;
          (void)cached_has_bits;

          // repeated .tensorflow.serving.InferenceTask tasks = 1;
          for (unsigned i = 0, n = static_cast<unsigned>(
                                   this_._internal_tasks_size());
               i < n; i++) {
            const auto& repfield = this_._internal_tasks().Get(i);
            target =
                ::google::protobuf::internal::WireFormatLite::InternalWriteMessage(
                    1, repfield, repfield.GetCachedSize(),
                    target, stream);
          }

          cached_has_bits = this_._impl_._has_bits_[0];
          // .tensorflow.serving.Input input = 2;
          if (cached_has_bits & 0x00000001u) {
            target = ::google::protobuf::internal::WireFormatLite::InternalWriteMessage(
                2, *this_._impl_.input_, this_._impl_.input_->GetCachedSize(), target,
                stream);
          }

          if (PROTOBUF_PREDICT_FALSE(this_._internal_metadata_.have_unknown_fields())) {
            target =
                ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
                    this_._internal_metadata_.unknown_fields<::google::protobuf::UnknownFieldSet>(::google::protobuf::UnknownFieldSet::default_instance), target, stream);
          }
          // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.MultiInferenceRequest)
          return target;
        }

#if defined(PROTOBUF_CUSTOM_VTABLE)
        ::size_t MultiInferenceRequest::ByteSizeLong(const MessageLite& base) {
          const MultiInferenceRequest& this_ = static_cast<const MultiInferenceRequest&>(base);
#else   // PROTOBUF_CUSTOM_VTABLE
        ::size_t MultiInferenceRequest::ByteSizeLong() const {
          const MultiInferenceRequest& this_ = *this;
#endif  // PROTOBUF_CUSTOM_VTABLE
          // @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.MultiInferenceRequest)
          ::size_t total_size = 0;

          ::uint32_t cached_has_bits = 0;
          // Prevent compiler warnings about cached_has_bits being unused
          (void)cached_has_bits;

          ::_pbi::Prefetch5LinesFrom7Lines(&this_);
           {
            // repeated .tensorflow.serving.InferenceTask tasks = 1;
            {
              total_size += 1UL * this_._internal_tasks_size();
              for (const auto& msg : this_._internal_tasks()) {
                total_size += ::google::protobuf::internal::WireFormatLite::MessageSize(msg);
              }
            }
          }
           {
            // .tensorflow.serving.Input input = 2;
            cached_has_bits = this_._impl_._has_bits_[0];
            if (cached_has_bits & 0x00000001u) {
              total_size += 1 +
                            ::google::protobuf::internal::WireFormatLite::MessageSize(*this_._impl_.input_);
            }
          }
          return this_.MaybeComputeUnknownFieldsSize(total_size,
                                                     &this_._impl_._cached_size_);
        }

void MultiInferenceRequest::MergeImpl(::google::protobuf::MessageLite& to_msg, const ::google::protobuf::MessageLite& from_msg) {
  auto* const _this = static_cast<MultiInferenceRequest*>(&to_msg);
  auto& from = static_cast<const MultiInferenceRequest&>(from_msg);
  ::google::protobuf::Arena* arena = _this->GetArena();
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.MultiInferenceRequest)
  ABSL_DCHECK_NE(&from, _this);
  ::uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  _this->_internal_mutable_tasks()->MergeFrom(
      from._internal_tasks());
  cached_has_bits = from._impl_._has_bits_[0];
  if (cached_has_bits & 0x00000001u) {
    ABSL_DCHECK(from._impl_.input_ != nullptr);
    if (_this->_impl_.input_ == nullptr) {
      _this->_impl_.input_ =
          ::google::protobuf::Message::CopyConstruct<::tensorflow::serving::Input>(arena, *from._impl_.input_);
    } else {
      _this->_impl_.input_->MergeFrom(*from._impl_.input_);
    }
  }
  _this->_impl_._has_bits_[0] |= cached_has_bits;
  _this->_internal_metadata_.MergeFrom<::google::protobuf::UnknownFieldSet>(from._internal_metadata_);
}

void MultiInferenceRequest::CopyFrom(const MultiInferenceRequest& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.MultiInferenceRequest)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}


void MultiInferenceRequest::InternalSwap(MultiInferenceRequest* PROTOBUF_RESTRICT other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  swap(_impl_._has_bits_[0], other->_impl_._has_bits_[0]);
  _impl_.tasks_.InternalSwap(&other->_impl_.tasks_);
  swap(_impl_.input_, other->_impl_.input_);
}

::google::protobuf::Metadata MultiInferenceRequest::GetMetadata() const {
  return ::google::protobuf::Message::GetMetadataImpl(GetClassData()->full());
}
// ===================================================================

class MultiInferenceResponse::_Internal {
 public:
};

MultiInferenceResponse::MultiInferenceResponse(::google::protobuf::Arena* arena)
#if defined(PROTOBUF_CUSTOM_VTABLE)
    : ::google::protobuf::Message(arena, _class_data_.base()) {
#else   // PROTOBUF_CUSTOM_VTABLE
    : ::google::protobuf::Message(arena) {
#endif  // PROTOBUF_CUSTOM_VTABLE
  SharedCtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.MultiInferenceResponse)
}
inline PROTOBUF_NDEBUG_INLINE MultiInferenceResponse::Impl_::Impl_(
    ::google::protobuf::internal::InternalVisibility visibility, ::google::protobuf::Arena* arena,
    const Impl_& from, const ::tensorflow::serving::MultiInferenceResponse& from_msg)
      : results_{visibility, arena, from.results_},
        _cached_size_{0} {}

MultiInferenceResponse::MultiInferenceResponse(
    ::google::protobuf::Arena* arena,
    const MultiInferenceResponse& from)
#if defined(PROTOBUF_CUSTOM_VTABLE)
    : ::google::protobuf::Message(arena, _class_data_.base()) {
#else   // PROTOBUF_CUSTOM_VTABLE
    : ::google::protobuf::Message(arena) {
#endif  // PROTOBUF_CUSTOM_VTABLE
  MultiInferenceResponse* const _this = this;
  (void)_this;
  _internal_metadata_.MergeFrom<::google::protobuf::UnknownFieldSet>(
      from._internal_metadata_);
  new (&_impl_) Impl_(internal_visibility(), arena, from._impl_, from);

  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.MultiInferenceResponse)
}
inline PROTOBUF_NDEBUG_INLINE MultiInferenceResponse::Impl_::Impl_(
    ::google::protobuf::internal::InternalVisibility visibility,
    ::google::protobuf::Arena* arena)
      : results_{visibility, arena},
        _cached_size_{0} {}

inline void MultiInferenceResponse::SharedCtor(::_pb::Arena* arena) {
  new (&_impl_) Impl_(internal_visibility(), arena);
}
MultiInferenceResponse::~MultiInferenceResponse() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.MultiInferenceResponse)
  SharedDtor(*this);
}
inline void MultiInferenceResponse::SharedDtor(MessageLite& self) {
  MultiInferenceResponse& this_ = static_cast<MultiInferenceResponse&>(self);
  this_._internal_metadata_.Delete<::google::protobuf::UnknownFieldSet>();
  ABSL_DCHECK(this_.GetArena() == nullptr);
  this_._impl_.~Impl_();
}

inline void* MultiInferenceResponse::PlacementNew_(const void*, void* mem,
                                        ::google::protobuf::Arena* arena) {
  return ::new (mem) MultiInferenceResponse(arena);
}
constexpr auto MultiInferenceResponse::InternalNewImpl_() {
  constexpr auto arena_bits = ::google::protobuf::internal::EncodePlacementArenaOffsets({
      PROTOBUF_FIELD_OFFSET(MultiInferenceResponse, _impl_.results_) +
          decltype(MultiInferenceResponse::_impl_.results_)::
              InternalGetArenaOffset(
                  ::google::protobuf::Message::internal_visibility()),
  });
  if (arena_bits.has_value()) {
    return ::google::protobuf::internal::MessageCreator::ZeroInit(
        sizeof(MultiInferenceResponse), alignof(MultiInferenceResponse), *arena_bits);
  } else {
    return ::google::protobuf::internal::MessageCreator(&MultiInferenceResponse::PlacementNew_,
                                 sizeof(MultiInferenceResponse),
                                 alignof(MultiInferenceResponse));
  }
}
PROTOBUF_CONSTINIT
PROTOBUF_ATTRIBUTE_INIT_PRIORITY1
const ::google::protobuf::internal::ClassDataFull MultiInferenceResponse::_class_data_ = {
    ::google::protobuf::internal::ClassData{
        &_MultiInferenceResponse_default_instance_._instance,
        &_table_.header,
        nullptr,  // OnDemandRegisterArenaDtor
        nullptr,  // IsInitialized
        &MultiInferenceResponse::MergeImpl,
        ::google::protobuf::Message::GetNewImpl<MultiInferenceResponse>(),
#if defined(PROTOBUF_CUSTOM_VTABLE)
        &MultiInferenceResponse::SharedDtor,
        ::google::protobuf::Message::GetClearImpl<MultiInferenceResponse>(), &MultiInferenceResponse::ByteSizeLong,
            &MultiInferenceResponse::_InternalSerialize,
#endif  // PROTOBUF_CUSTOM_VTABLE
        PROTOBUF_FIELD_OFFSET(MultiInferenceResponse, _impl_._cached_size_),
        false,
    },
    &MultiInferenceResponse::kDescriptorMethods,
    &descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto,
    nullptr,  // tracker
};
const ::google::protobuf::internal::ClassData* MultiInferenceResponse::GetClassData() const {
  ::google::protobuf::internal::PrefetchToLocalCache(&_class_data_);
  ::google::protobuf::internal::PrefetchToLocalCache(_class_data_.tc_table);
  return _class_data_.base();
}
PROTOBUF_CONSTINIT PROTOBUF_ATTRIBUTE_INIT_PRIORITY1
const ::_pbi::TcParseTable<0, 1, 1, 0, 2> MultiInferenceResponse::_table_ = {
  {
    0,  // no _has_bits_
    0, // no _extensions_
    1, 0,  // max_field_number, fast_idx_mask
    offsetof(decltype(_table_), field_lookup_table),
    4294967294,  // skipmap
    offsetof(decltype(_table_), field_entries),
    1,  // num_field_entries
    1,  // num_aux_entries
    offsetof(decltype(_table_), aux_entries),
    _class_data_.base(),
    nullptr,  // post_loop_handler
    ::_pbi::TcParser::GenericFallback,  // fallback
    #ifdef PROTOBUF_PREFETCH_PARSE_TABLE
    ::_pbi::TcParser::GetTable<::tensorflow::serving::MultiInferenceResponse>(),  // to_prefetch
    #endif  // PROTOBUF_PREFETCH_PARSE_TABLE
  }, {{
    // repeated .tensorflow.serving.InferenceResult results = 1;
    {::_pbi::TcParser::FastMtR1,
     {10, 63, 0, PROTOBUF_FIELD_OFFSET(MultiInferenceResponse, _impl_.results_)}},
  }}, {{
    65535, 65535
  }}, {{
    // repeated .tensorflow.serving.InferenceResult results = 1;
    {PROTOBUF_FIELD_OFFSET(MultiInferenceResponse, _impl_.results_), 0, 0,
    (0 | ::_fl::kFcRepeated | ::_fl::kMessage | ::_fl::kTvTable)},
  }}, {{
    {::_pbi::TcParser::GetTable<::tensorflow::serving::InferenceResult>()},
  }}, {{
  }},
};

PROTOBUF_NOINLINE void MultiInferenceResponse::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.MultiInferenceResponse)
  ::google::protobuf::internal::TSanWrite(&_impl_);
  ::uint32_t cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _impl_.results_.Clear();
  _internal_metadata_.Clear<::google::protobuf::UnknownFieldSet>();
}

#if defined(PROTOBUF_CUSTOM_VTABLE)
        ::uint8_t* MultiInferenceResponse::_InternalSerialize(
            const MessageLite& base, ::uint8_t* target,
            ::google::protobuf::io::EpsCopyOutputStream* stream) {
          const MultiInferenceResponse& this_ = static_cast<const MultiInferenceResponse&>(base);
#else   // PROTOBUF_CUSTOM_VTABLE
        ::uint8_t* MultiInferenceResponse::_InternalSerialize(
            ::uint8_t* target,
            ::google::protobuf::io::EpsCopyOutputStream* stream) const {
          const MultiInferenceResponse& this_ = *this;
#endif  // PROTOBUF_CUSTOM_VTABLE
          // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.MultiInferenceResponse)
          ::uint32_t cached_has_bits = 0;
          (void)cached_has_bits;

          // repeated .tensorflow.serving.InferenceResult results = 1;
          for (unsigned i = 0, n = static_cast<unsigned>(
                                   this_._internal_results_size());
               i < n; i++) {
            const auto& repfield = this_._internal_results().Get(i);
            target =
                ::google::protobuf::internal::WireFormatLite::InternalWriteMessage(
                    1, repfield, repfield.GetCachedSize(),
                    target, stream);
          }

          if (PROTOBUF_PREDICT_FALSE(this_._internal_metadata_.have_unknown_fields())) {
            target =
                ::_pbi::WireFormat::InternalSerializeUnknownFieldsToArray(
                    this_._internal_metadata_.unknown_fields<::google::protobuf::UnknownFieldSet>(::google::protobuf::UnknownFieldSet::default_instance), target, stream);
          }
          // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.MultiInferenceResponse)
          return target;
        }

#if defined(PROTOBUF_CUSTOM_VTABLE)
        ::size_t MultiInferenceResponse::ByteSizeLong(const MessageLite& base) {
          const MultiInferenceResponse& this_ = static_cast<const MultiInferenceResponse&>(base);
#else   // PROTOBUF_CUSTOM_VTABLE
        ::size_t MultiInferenceResponse::ByteSizeLong() const {
          const MultiInferenceResponse& this_ = *this;
#endif  // PROTOBUF_CUSTOM_VTABLE
          // @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.MultiInferenceResponse)
          ::size_t total_size = 0;

          ::uint32_t cached_has_bits = 0;
          // Prevent compiler warnings about cached_has_bits being unused
          (void)cached_has_bits;

          ::_pbi::Prefetch5LinesFrom7Lines(&this_);
           {
            // repeated .tensorflow.serving.InferenceResult results = 1;
            {
              total_size += 1UL * this_._internal_results_size();
              for (const auto& msg : this_._internal_results()) {
                total_size += ::google::protobuf::internal::WireFormatLite::MessageSize(msg);
              }
            }
          }
          return this_.MaybeComputeUnknownFieldsSize(total_size,
                                                     &this_._impl_._cached_size_);
        }

void MultiInferenceResponse::MergeImpl(::google::protobuf::MessageLite& to_msg, const ::google::protobuf::MessageLite& from_msg) {
  auto* const _this = static_cast<MultiInferenceResponse*>(&to_msg);
  auto& from = static_cast<const MultiInferenceResponse&>(from_msg);
  // @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.MultiInferenceResponse)
  ABSL_DCHECK_NE(&from, _this);
  ::uint32_t cached_has_bits = 0;
  (void) cached_has_bits;

  _this->_internal_mutable_results()->MergeFrom(
      from._internal_results());
  _this->_internal_metadata_.MergeFrom<::google::protobuf::UnknownFieldSet>(from._internal_metadata_);
}

void MultiInferenceResponse::CopyFrom(const MultiInferenceResponse& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.MultiInferenceResponse)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}


void MultiInferenceResponse::InternalSwap(MultiInferenceResponse* PROTOBUF_RESTRICT other) {
  using std::swap;
  _internal_metadata_.InternalSwap(&other->_internal_metadata_);
  _impl_.results_.InternalSwap(&other->_impl_.results_);
}

::google::protobuf::Metadata MultiInferenceResponse::GetMetadata() const {
  return ::google::protobuf::Message::GetMetadataImpl(GetClassData()->full());
}
// @@protoc_insertion_point(namespace_scope)
}  // namespace serving
}  // namespace tensorflow
namespace google {
namespace protobuf {
}  // namespace protobuf
}  // namespace google
// @@protoc_insertion_point(global_scope)
PROTOBUF_ATTRIBUTE_INIT_PRIORITY2 static ::std::false_type
    _static_init2_ PROTOBUF_UNUSED =
        (::_pbi::AddDescriptors(&descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto),
         ::std::false_type{});
#include "google/protobuf/port_undef.inc"
